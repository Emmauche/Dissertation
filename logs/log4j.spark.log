20/08/17 06:54:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/08/17 06:54:50 INFO SecurityManager: Changing view acls to: emmanuel
20/08/17 06:54:50 INFO SecurityManager: Changing modify acls to: emmanuel
20/08/17 06:54:50 INFO SecurityManager: Changing view acls groups to: 
20/08/17 06:54:50 INFO SecurityManager: Changing modify acls groups to: 
20/08/17 06:54:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(emmanuel); groups with view permissions: Set(); users  with modify permissions: Set(emmanuel); groups with modify permissions: Set()
20/08/17 06:54:52 INFO HiveConf: Found configuration file file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 06:54:53 INFO SparkContext: Running Spark version 3.0.0
20/08/17 06:54:53 INFO ResourceUtils: ==============================================================
20/08/17 06:54:53 INFO ResourceUtils: Resources for spark.driver:

20/08/17 06:54:53 INFO ResourceUtils: ==============================================================
20/08/17 06:54:53 INFO SparkContext: Submitted application: sparklyr
20/08/17 06:54:53 INFO SecurityManager: Changing view acls to: emmanuel
20/08/17 06:54:53 INFO SecurityManager: Changing modify acls to: emmanuel
20/08/17 06:54:53 INFO SecurityManager: Changing view acls groups to: 
20/08/17 06:54:53 INFO SecurityManager: Changing modify acls groups to: 
20/08/17 06:54:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(emmanuel); groups with view permissions: Set(); users  with modify permissions: Set(emmanuel); groups with modify permissions: Set()
20/08/17 06:54:53 INFO Utils: Successfully started service 'sparkDriver' on port 36981.
20/08/17 06:54:53 INFO SparkEnv: Registering MapOutputTracker
20/08/17 06:54:53 INFO SparkEnv: Registering BlockManagerMaster
20/08/17 06:54:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/08/17 06:54:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/08/17 06:54:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/08/17 06:54:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c0fb6bf8-c5cb-49e2-a547-91d0cc668f26
20/08/17 06:54:54 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/08/17 06:54:54 INFO SparkEnv: Registering OutputCommitCoordinator
20/08/17 06:54:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/08/17 06:54:54 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
20/08/17 06:54:54 INFO SparkContext: Added JAR file:/home/emmanuel/R/x86_64-pc-linux-gnu-library/3.4/sparklyr/java/sparklyr-3.0-2.12.jar at spark://localhost:36981/jars/sparklyr-3.0-2.12.jar with timestamp 1597661694917
20/08/17 06:54:55 INFO Executor: Starting executor ID driver on host localhost
20/08/17 06:54:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46113.
20/08/17 06:54:55 INFO NettyBlockTransferService: Server created on localhost:46113
20/08/17 06:54:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/08/17 06:54:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 46113, None)
20/08/17 06:54:55 INFO BlockManagerMasterEndpoint: Registering block manager localhost:46113 with 912.3 MiB RAM, BlockManagerId(driver, localhost, 46113, None)
20/08/17 06:54:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 46113, None)
20/08/17 06:54:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 46113, None)
20/08/17 06:54:56 INFO SharedState: loading hive config file: file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 06:54:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/emmanuel/R/Dissertation/spark-warehouse').
20/08/17 06:54:56 INFO SharedState: Warehouse path is 'file:/home/emmanuel/R/Dissertation/spark-warehouse'.
20/08/17 06:55:02 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/08/17 06:55:02 INFO HiveConf: Found configuration file file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 06:55:03 INFO SessionState: Created HDFS directory: /tmp/hive/emmanuel
20/08/17 06:55:03 INFO SessionState: Created local directory: /tmp/emmanuel
20/08/17 06:55:03 INFO SessionState: Created HDFS directory: /tmp/hive/emmanuel/9b151b0e-3b95-4120-b498-4a43a17cf414
20/08/17 06:55:03 INFO SessionState: Created local directory: /tmp/emmanuel/9b151b0e-3b95-4120-b498-4a43a17cf414
20/08/17 06:55:03 INFO SessionState: Created HDFS directory: /tmp/hive/emmanuel/9b151b0e-3b95-4120-b498-4a43a17cf414/_tmp_space.db
20/08/17 06:55:03 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is file:/home/emmanuel/R/Dissertation/spark-warehouse
20/08/17 06:55:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/08/17 06:55:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/08/17 06:55:05 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/08/17 06:55:05 INFO ObjectStore: ObjectStore, initialize called
20/08/17 06:55:06 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/08/17 06:55:06 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/08/17 06:55:11 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/08/17 06:55:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/08/17 06:55:16 INFO ObjectStore: Initialized ObjectStore
20/08/17 06:55:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/08/17 06:55:17 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore emmanuel@127.0.1.1
20/08/17 06:55:17 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/08/17 06:55:18 INFO HiveMetaStore: Added admin role in metastore
20/08/17 06:55:18 INFO HiveMetaStore: Added public role in metastore
20/08/17 06:55:18 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/08/17 06:55:19 INFO HiveMetaStore: 0: get_all_functions
20/08/17 06:55:19 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_all_functions	
20/08/17 06:55:19 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:55:19 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:55:19 INFO HiveMetaStore: 0: get_database: global_temp
20/08/17 06:55:19 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/08/17 06:55:19 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/08/17 06:55:19 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:55:19 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:55:19 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:55:19 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:55:19 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 06:55:19 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 06:55:23 INFO CodeGenerator: Code generated in 627.188941 ms
20/08/17 06:55:23 INFO CodeGenerator: Code generated in 33.433039 ms
20/08/17 06:55:23 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 06:55:23 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/08/17 06:55:24 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/08/17 06:55:24 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/08/17 06:55:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/08/17 06:55:24 INFO DAGScheduler: Missing parents: List()
20/08/17 06:55:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/08/17 06:55:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/08/17 06:55:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/08/17 06:55:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:46113 (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:55:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/08/17 06:55:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:55:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/08/17 06:55:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/08/17 06:55:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/08/17 06:55:25 INFO Executor: Fetching spark://localhost:36981/jars/sparklyr-3.0-2.12.jar with timestamp 1597661694917
20/08/17 06:55:25 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:36981 after 94 ms (0 ms spent in bootstraps)
20/08/17 06:55:25 INFO Utils: Fetching spark://localhost:36981/jars/sparklyr-3.0-2.12.jar to /tmp/spark-33e1027e-3149-471c-ac26-4ab9e1af599a/userFiles-1821dd87-7829-4648-875d-01631a2f55ce/fetchFileTemp1003143730700941398.tmp
20/08/17 06:55:25 INFO Executor: Adding file:/tmp/spark-33e1027e-3149-471c-ac26-4ab9e1af599a/userFiles-1821dd87-7829-4648-875d-01631a2f55ce/sparklyr-3.0-2.12.jar to class loader
20/08/17 06:55:26 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 06:55:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
20/08/17 06:55:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/08/17 06:55:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1962 ms on localhost (executor driver) (1/1)
20/08/17 06:55:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/08/17 06:55:27 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2.879 s
20/08/17 06:55:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:55:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/08/17 06:55:27 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 3.186853 s
20/08/17 06:55:54 INFO SparkContext: Invoking stop() from shutdown hook
20/08/17 06:55:54 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
20/08/17 06:55:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/08/17 06:55:55 INFO MemoryStore: MemoryStore cleared
20/08/17 06:55:55 INFO BlockManager: BlockManager stopped
20/08/17 06:55:55 INFO BlockManagerMaster: BlockManagerMaster stopped
20/08/17 06:55:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/08/17 06:55:55 INFO SparkContext: Successfully stopped SparkContext
20/08/17 06:55:55 INFO ShutdownHookManager: Shutdown hook called
20/08/17 06:55:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-bfe32aca-2db8-47db-9565-b5cad45992d6
20/08/17 06:55:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-33e1027e-3149-471c-ac26-4ab9e1af599a
20/08/17 06:57:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/08/17 06:57:26 INFO SecurityManager: Changing view acls to: emmanuel
20/08/17 06:57:26 INFO SecurityManager: Changing modify acls to: emmanuel
20/08/17 06:57:26 INFO SecurityManager: Changing view acls groups to: 
20/08/17 06:57:26 INFO SecurityManager: Changing modify acls groups to: 
20/08/17 06:57:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(emmanuel); groups with view permissions: Set(); users  with modify permissions: Set(emmanuel); groups with modify permissions: Set()
20/08/17 06:57:28 INFO HiveConf: Found configuration file file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 06:57:28 INFO SparkContext: Running Spark version 3.0.0
20/08/17 06:57:28 INFO ResourceUtils: ==============================================================
20/08/17 06:57:28 INFO ResourceUtils: Resources for spark.driver:

20/08/17 06:57:28 INFO ResourceUtils: ==============================================================
20/08/17 06:57:28 INFO SparkContext: Submitted application: sparklyr
20/08/17 06:57:29 INFO SecurityManager: Changing view acls to: emmanuel
20/08/17 06:57:29 INFO SecurityManager: Changing modify acls to: emmanuel
20/08/17 06:57:29 INFO SecurityManager: Changing view acls groups to: 
20/08/17 06:57:29 INFO SecurityManager: Changing modify acls groups to: 
20/08/17 06:57:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(emmanuel); groups with view permissions: Set(); users  with modify permissions: Set(emmanuel); groups with modify permissions: Set()
20/08/17 06:57:29 INFO Utils: Successfully started service 'sparkDriver' on port 39869.
20/08/17 06:57:29 INFO SparkEnv: Registering MapOutputTracker
20/08/17 06:57:29 INFO SparkEnv: Registering BlockManagerMaster
20/08/17 06:57:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/08/17 06:57:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/08/17 06:57:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/08/17 06:57:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-794698f3-8235-4058-8c0f-0cd3bb368b08
20/08/17 06:57:29 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/08/17 06:57:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/08/17 06:57:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/08/17 06:57:30 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
20/08/17 06:57:30 INFO SparkContext: Added JAR file:/home/emmanuel/R/x86_64-pc-linux-gnu-library/3.4/sparklyr/java/sparklyr-3.0-2.12.jar at spark://localhost:39869/jars/sparklyr-3.0-2.12.jar with timestamp 1597661850525
20/08/17 06:57:30 INFO Executor: Starting executor ID driver on host localhost
20/08/17 06:57:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44647.
20/08/17 06:57:30 INFO NettyBlockTransferService: Server created on localhost:44647
20/08/17 06:57:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/08/17 06:57:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 44647, None)
20/08/17 06:57:30 INFO BlockManagerMasterEndpoint: Registering block manager localhost:44647 with 912.3 MiB RAM, BlockManagerId(driver, localhost, 44647, None)
20/08/17 06:57:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 44647, None)
20/08/17 06:57:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 44647, None)
20/08/17 06:57:32 INFO SharedState: loading hive config file: file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 06:57:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/emmanuel/R/Dissertation/spark-warehouse').
20/08/17 06:57:32 INFO SharedState: Warehouse path is 'file:/home/emmanuel/R/Dissertation/spark-warehouse'.
20/08/17 06:57:36 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/08/17 06:57:36 INFO HiveConf: Found configuration file file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 06:57:38 INFO SessionState: Created HDFS directory: /tmp/hive/emmanuel/476cc53d-1b22-460a-b6b2-947a2a3e1ee0
20/08/17 06:57:38 INFO SessionState: Created local directory: /tmp/emmanuel/476cc53d-1b22-460a-b6b2-947a2a3e1ee0
20/08/17 06:57:38 INFO SessionState: Created HDFS directory: /tmp/hive/emmanuel/476cc53d-1b22-460a-b6b2-947a2a3e1ee0/_tmp_space.db
20/08/17 06:57:38 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is file:/home/emmanuel/R/Dissertation/spark-warehouse
20/08/17 06:57:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/08/17 06:57:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/08/17 06:57:39 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/08/17 06:57:39 INFO ObjectStore: ObjectStore, initialize called
20/08/17 06:57:39 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/08/17 06:57:39 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/08/17 06:57:42 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/08/17 06:57:46 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/08/17 06:57:46 INFO ObjectStore: Initialized ObjectStore
20/08/17 06:57:46 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/08/17 06:57:46 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore emmanuel@127.0.1.1
20/08/17 06:57:46 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/08/17 06:57:46 INFO HiveMetaStore: Added admin role in metastore
20/08/17 06:57:46 INFO HiveMetaStore: Added public role in metastore
20/08/17 06:57:47 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/08/17 06:57:47 INFO HiveMetaStore: 0: get_all_functions
20/08/17 06:57:47 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_all_functions	
20/08/17 06:57:47 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:57:47 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:57:47 INFO HiveMetaStore: 0: get_database: global_temp
20/08/17 06:57:47 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/08/17 06:57:47 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/08/17 06:57:47 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:57:47 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:57:47 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:57:47 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:57:47 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 06:57:47 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 06:57:50 INFO CodeGenerator: Code generated in 455.939551 ms
20/08/17 06:57:50 INFO CodeGenerator: Code generated in 23.602083 ms
20/08/17 06:57:51 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 06:57:51 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/08/17 06:57:51 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/08/17 06:57:51 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/08/17 06:57:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/08/17 06:57:51 INFO DAGScheduler: Missing parents: List()
20/08/17 06:57:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/08/17 06:57:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/08/17 06:57:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/08/17 06:57:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:57:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/08/17 06:57:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:57:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/08/17 06:57:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/08/17 06:57:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/08/17 06:57:52 INFO Executor: Fetching spark://localhost:39869/jars/sparklyr-3.0-2.12.jar with timestamp 1597661850525
20/08/17 06:57:52 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:39869 after 103 ms (0 ms spent in bootstraps)
20/08/17 06:57:52 INFO Utils: Fetching spark://localhost:39869/jars/sparklyr-3.0-2.12.jar to /tmp/spark-9ce82bfc-a42a-417a-8c7f-0cdd69c69ed5/userFiles-a4fdbbf6-5978-4c26-a250-f1feebb668dd/fetchFileTemp317972460886080313.tmp
20/08/17 06:57:52 INFO Executor: Adding file:/tmp/spark-9ce82bfc-a42a-417a-8c7f-0cdd69c69ed5/userFiles-a4fdbbf6-5978-4c26-a250-f1feebb668dd/sparklyr-3.0-2.12.jar to class loader
20/08/17 06:57:53 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 06:57:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
20/08/17 06:57:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/08/17 06:57:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1542 ms on localhost (executor driver) (1/1)
20/08/17 06:57:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/08/17 06:57:53 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2.305 s
20/08/17 06:57:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:57:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/08/17 06:57:53 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2.566024 s
20/08/17 06:58:15 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:58:15 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:58:15 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:58:15 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:58:15 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 06:58:15 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 06:58:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:58:16 INFO SparkContext: Starting job: collect at utils.scala:41
20/08/17 06:58:16 INFO DAGScheduler: Job 1 finished: collect at utils.scala:41, took 0.000267 s
20/08/17 06:58:16 INFO CodeGenerator: Code generated in 19.795636 ms
20/08/17 06:58:17 INFO CodeGenerator: Code generated in 31.630666 ms
20/08/17 06:58:17 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
20/08/17 06:58:17 INFO DAGScheduler: Registering RDD 21 (sql at NativeMethodAccessorImpl.java:0) as input to shuffle 1
20/08/17 06:58:17 INFO DAGScheduler: Got job 2 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 06:58:17 INFO DAGScheduler: Final stage: ResultStage 3 (sql at NativeMethodAccessorImpl.java:0)
20/08/17 06:58:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/08/17 06:58:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/08/17 06:58:17 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[21] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 06:58:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 37.2 KiB, free 912.3 MiB)
20/08/17 06:58:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 912.3 MiB)
20/08/17 06:58:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:44647 (size: 13.2 KiB, free: 912.3 MiB)
20/08/17 06:58:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[21] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
20/08/17 06:58:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 06:58:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
20/08/17 06:58:17 INFO CodeGenerator: Code generated in 177.891716 ms
20/08/17 06:58:17 INFO MemoryStore: Block rdd_16_0 stored as values in memory (estimated size 4.2 KiB, free 912.2 MiB)
20/08/17 06:58:17 INFO BlockManagerInfo: Added rdd_16_0 in memory on localhost:44647 (size: 4.2 KiB, free: 912.3 MiB)
20/08/17 06:58:17 INFO CodeGenerator: Code generated in 16.248407 ms
20/08/17 06:58:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 2231 bytes result sent to driver
20/08/17 06:58:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 777 ms on localhost (executor driver) (1/1)
20/08/17 06:58:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/08/17 06:58:18 INFO DAGScheduler: ShuffleMapStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 0.896 s
20/08/17 06:58:18 INFO DAGScheduler: looking for newly runnable stages
20/08/17 06:58:18 INFO DAGScheduler: running: Set()
20/08/17 06:58:18 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/08/17 06:58:18 INFO DAGScheduler: failed: Set()
20/08/17 06:58:18 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[24] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 06:58:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 912.2 MiB)
20/08/17 06:58:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.2 MiB)
20/08/17 06:58:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:58:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[24] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
20/08/17 06:58:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 06:58:18 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
20/08/17 06:58:18 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 06:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
20/08/17 06:58:18 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2648 bytes result sent to driver
20/08/17 06:58:18 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 54 ms on localhost (executor driver) (1/1)
20/08/17 06:58:18 INFO DAGScheduler: ResultStage 3 (sql at NativeMethodAccessorImpl.java:0) finished in 0.083 s
20/08/17 06:58:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:58:18 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/08/17 06:58:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
20/08/17 06:58:18 INFO DAGScheduler: Job 2 finished: sql at NativeMethodAccessorImpl.java:0, took 1.075032 s
20/08/17 06:58:19 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 06:58:19 INFO DAGScheduler: Registering RDD 29 (collect at utils.scala:116) as input to shuffle 2
20/08/17 06:58:19 INFO DAGScheduler: Got job 3 (collect at utils.scala:116) with 1 output partitions
20/08/17 06:58:19 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:116)
20/08/17 06:58:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/08/17 06:58:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)
20/08/17 06:58:19 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[29] at collect at utils.scala:116), which has no missing parents
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 37.2 KiB, free 912.2 MiB)
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 912.2 MiB)
20/08/17 06:58:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:44647 (size: 13.2 KiB, free: 912.3 MiB)
20/08/17 06:58:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[29] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:19 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
20/08/17 06:58:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 06:58:19 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
20/08/17 06:58:19 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 06:58:19 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2274 bytes result sent to driver
20/08/17 06:58:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 29 ms on localhost (executor driver) (1/1)
20/08/17 06:58:19 INFO DAGScheduler: ShuffleMapStage 4 (collect at utils.scala:116) finished in 0.047 s
20/08/17 06:58:19 INFO DAGScheduler: looking for newly runnable stages
20/08/17 06:58:19 INFO DAGScheduler: running: Set()
20/08/17 06:58:19 INFO DAGScheduler: waiting: Set(ResultStage 5)
20/08/17 06:58:19 INFO DAGScheduler: failed: Set()
20/08/17 06:58:19 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[32] at collect at utils.scala:116), which has no missing parents
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.1 KiB, free 912.2 MiB)
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.2 MiB)
20/08/17 06:58:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:58:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/08/17 06:58:19 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[32] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:19 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
20/08/17 06:58:19 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 06:58:19 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
20/08/17 06:58:19 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 06:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/08/17 06:58:19 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 2648 bytes result sent to driver
20/08/17 06:58:19 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 12 ms on localhost (executor driver) (1/1)
20/08/17 06:58:19 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/08/17 06:58:19 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:116) finished in 0.025 s
20/08/17 06:58:19 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:58:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
20/08/17 06:58:19 INFO DAGScheduler: Job 3 finished: collect at utils.scala:116, took 0.087139 s
20/08/17 06:58:19 INFO CodeGenerator: Code generated in 7.592772 ms
20/08/17 06:58:19 INFO CodeGenerator: Code generated in 50.70017 ms
20/08/17 06:58:19 INFO CodeGenerator: Code generated in 56.71499 ms
20/08/17 06:58:19 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 06:58:19 INFO DAGScheduler: Registering RDD 37 (count at utils.scala:114) as input to shuffle 3
20/08/17 06:58:19 INFO DAGScheduler: Got job 4 (count at utils.scala:114) with 1 output partitions
20/08/17 06:58:19 INFO DAGScheduler: Final stage: ResultStage 7 (count at utils.scala:114)
20/08/17 06:58:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
20/08/17 06:58:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)
20/08/17 06:58:19 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[37] at count at utils.scala:114), which has no missing parents
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.6 KiB, free 912.1 MiB)
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 912.1 MiB)
20/08/17 06:58:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:44647 (size: 13.0 KiB, free: 912.2 MiB)
20/08/17 06:58:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[37] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:19 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
20/08/17 06:58:19 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 06:58:19 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
20/08/17 06:58:19 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 06:58:19 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 2274 bytes result sent to driver
20/08/17 06:58:19 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 17 ms on localhost (executor driver) (1/1)
20/08/17 06:58:19 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
20/08/17 06:58:19 INFO DAGScheduler: ShuffleMapStage 6 (count at utils.scala:114) finished in 0.029 s
20/08/17 06:58:19 INFO DAGScheduler: looking for newly runnable stages
20/08/17 06:58:19 INFO DAGScheduler: running: Set()
20/08/17 06:58:19 INFO DAGScheduler: waiting: Set(ResultStage 7)
20/08/17 06:58:19 INFO DAGScheduler: failed: Set()
20/08/17 06:58:19 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[40] at count at utils.scala:114), which has no missing parents
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.1 KiB, free 912.1 MiB)
20/08/17 06:58:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.1 MiB)
20/08/17 06:58:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 06:58:19 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[40] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:19 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
20/08/17 06:58:19 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 06:58:19 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
20/08/17 06:58:19 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 06:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/08/17 06:58:19 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 2896 bytes result sent to driver
20/08/17 06:58:19 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 29 ms on localhost (executor driver) (1/1)
20/08/17 06:58:19 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
20/08/17 06:58:19 INFO DAGScheduler: ResultStage 7 (count at utils.scala:114) finished in 0.045 s
20/08/17 06:58:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:58:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
20/08/17 06:58:19 INFO DAGScheduler: Job 4 finished: count at utils.scala:114, took 0.086310 s
20/08/17 06:58:20 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:58:20 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:58:20 INFO HiveMetaStore: 0: get_database: default
20/08/17 06:58:20 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 06:58:20 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 06:58:20 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 06:58:20 INFO CodeGenerator: Code generated in 14.363949 ms
20/08/17 06:58:20 INFO CodeGenerator: Code generated in 73.22139 ms
20/08/17 06:58:21 INFO CodeGenerator: Code generated in 16.745894 ms
20/08/17 06:58:21 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 06:58:21 INFO DAGScheduler: Registering RDD 43 (count at utils.scala:114) as input to shuffle 4
20/08/17 06:58:21 INFO DAGScheduler: Got job 5 (count at utils.scala:114) with 1 output partitions
20/08/17 06:58:21 INFO DAGScheduler: Final stage: ResultStage 9 (count at utils.scala:114)
20/08/17 06:58:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
20/08/17 06:58:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)
20/08/17 06:58:21 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[43] at count at utils.scala:114), which has no missing parents
20/08/17 06:58:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.1 KiB, free 912.1 MiB)
20/08/17 06:58:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.1 MiB)
20/08/17 06:58:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 06:58:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[43] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:21 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
20/08/17 06:58:21 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7498 bytes)
20/08/17 06:58:21 INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
20/08/17 06:58:21 INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 1833 bytes result sent to driver
20/08/17 06:58:21 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 24 ms on localhost (executor driver) (1/1)
20/08/17 06:58:21 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
20/08/17 06:58:21 INFO DAGScheduler: ShuffleMapStage 8 (count at utils.scala:114) finished in 0.050 s
20/08/17 06:58:21 INFO DAGScheduler: looking for newly runnable stages
20/08/17 06:58:21 INFO DAGScheduler: running: Set()
20/08/17 06:58:21 INFO DAGScheduler: waiting: Set(ResultStage 9)
20/08/17 06:58:21 INFO DAGScheduler: failed: Set()
20/08/17 06:58:21 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[46] at count at utils.scala:114), which has no missing parents
20/08/17 06:58:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 10.1 KiB, free 912.1 MiB)
20/08/17 06:58:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.1 MiB)
20/08/17 06:58:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 06:58:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[46] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:21 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
20/08/17 06:58:21 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 06:58:21 INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
20/08/17 06:58:21 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 06:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 06:58:21 INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 2648 bytes result sent to driver
20/08/17 06:58:21 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 7 ms on localhost (executor driver) (1/1)
20/08/17 06:58:21 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
20/08/17 06:58:21 INFO DAGScheduler: ResultStage 9 (count at utils.scala:114) finished in 0.017 s
20/08/17 06:58:21 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:58:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
20/08/17 06:58:21 INFO DAGScheduler: Job 5 finished: count at utils.scala:114, took 0.093267 s
20/08/17 06:58:28 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:44647 in memory (size: 13.0 KiB, free: 912.2 MiB)
20/08/17 06:58:28 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 06:58:28 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:44647 in memory (size: 13.2 KiB, free: 912.3 MiB)
20/08/17 06:58:28 INFO CodeGenerator: Code generated in 67.972113 ms
20/08/17 06:58:28 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 06:58:28 INFO DAGScheduler: Got job 6 (collect at utils.scala:116) with 1 output partitions
20/08/17 06:58:28 INFO DAGScheduler: Final stage: ResultStage 10 (collect at utils.scala:116)
20/08/17 06:58:28 INFO DAGScheduler: Parents of final stage: List()
20/08/17 06:58:28 INFO DAGScheduler: Missing parents: List()
20/08/17 06:58:28 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[51] at collect at utils.scala:116), which has no missing parents
20/08/17 06:58:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 40.7 KiB, free 912.1 MiB)
20/08/17 06:58:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 912.1 MiB)
20/08/17 06:58:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:44647 (size: 12.8 KiB, free: 912.3 MiB)
20/08/17 06:58:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[51] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:28 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
20/08/17 06:58:28 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8649 bytes)
20/08/17 06:58:28 INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
20/08/17 06:58:28 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 06:58:28 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:58:28 INFO Executor: 1 block locks were not released by TID = 9:
[rdd_16_0]
20/08/17 06:58:28 INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 2269 bytes result sent to driver
20/08/17 06:58:28 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 136 ms on localhost (executor driver) (1/1)
20/08/17 06:58:28 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
20/08/17 06:58:28 INFO DAGScheduler: ResultStage 10 (collect at utils.scala:116) finished in 0.157 s
20/08/17 06:58:28 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:58:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
20/08/17 06:58:28 INFO DAGScheduler: Job 6 finished: collect at utils.scala:116, took 0.167036 s
20/08/17 06:58:28 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:58:28 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 06:58:29 INFO CodeGenerator: Code generated in 50.996517 ms
20/08/17 06:58:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:44647 in memory (size: 13.2 KiB, free: 912.3 MiB)
20/08/17 06:58:29 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 06:58:29 INFO CodeGenerator: Code generated in 71.378941 ms
20/08/17 06:58:29 INFO CodeGenerator: Code generated in 35.684654 ms
20/08/17 06:58:29 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 06:58:29 INFO DAGScheduler: Registering RDD 56 (count at utils.scala:114) as input to shuffle 5
20/08/17 06:58:29 INFO DAGScheduler: Got job 7 (count at utils.scala:114) with 1 output partitions
20/08/17 06:58:29 INFO DAGScheduler: Final stage: ResultStage 12 (count at utils.scala:114)
20/08/17 06:58:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
20/08/17 06:58:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 11)
20/08/17 06:58:29 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[56] at count at utils.scala:114), which has no missing parents
20/08/17 06:58:29 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 36.1 KiB, free 912.2 MiB)
20/08/17 06:58:29 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
20/08/17 06:58:29 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:44647 (size: 12.7 KiB, free: 912.3 MiB)
20/08/17 06:58:29 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[56] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:29 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
20/08/17 06:58:29 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 06:58:29 INFO Executor: Running task 0.0 in stage 11.0 (TID 10)
20/08/17 06:58:29 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 06:58:29 INFO Executor: 1 block locks were not released by TID = 10:
[rdd_16_0]
20/08/17 06:58:29 INFO Executor: Finished task 0.0 in stage 11.0 (TID 10). 2127 bytes result sent to driver
20/08/17 06:58:29 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 48 ms on localhost (executor driver) (1/1)
20/08/17 06:58:29 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
20/08/17 06:58:29 INFO DAGScheduler: ShuffleMapStage 11 (count at utils.scala:114) finished in 0.090 s
20/08/17 06:58:29 INFO DAGScheduler: looking for newly runnable stages
20/08/17 06:58:29 INFO DAGScheduler: running: Set()
20/08/17 06:58:29 INFO DAGScheduler: waiting: Set(ResultStage 12)
20/08/17 06:58:29 INFO DAGScheduler: failed: Set()
20/08/17 06:58:29 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[59] at count at utils.scala:114), which has no missing parents
20/08/17 06:58:29 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.5 KiB, free 912.2 MiB)
20/08/17 06:58:29 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.2 MiB)
20/08/17 06:58:29 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 06:58:29 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1200
20/08/17 06:58:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[59] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 06:58:29 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
20/08/17 06:58:29 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 06:58:29 INFO Executor: Running task 0.0 in stage 12.0 (TID 11)
20/08/17 06:58:29 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 06:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/08/17 06:58:29 INFO Executor: Finished task 0.0 in stage 12.0 (TID 11). 2471 bytes result sent to driver
20/08/17 06:58:29 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 25 ms on localhost (executor driver) (1/1)
20/08/17 06:58:29 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
20/08/17 06:58:29 INFO DAGScheduler: ResultStage 12 (count at utils.scala:114) finished in 0.049 s
20/08/17 06:58:29 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 06:58:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
20/08/17 06:58:29 INFO DAGScheduler: Job 7 finished: count at utils.scala:114, took 0.165171 s
20/08/17 07:00:17 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:00:17 INFO DAGScheduler: Registering RDD 64 (collect at utils.scala:116) as input to shuffle 6
20/08/17 07:00:17 INFO DAGScheduler: Got job 8 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:00:17 INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:116)
20/08/17 07:00:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
20/08/17 07:00:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)
20/08/17 07:00:17 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[64] at collect at utils.scala:116), which has no missing parents
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 37.2 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:44647 (size: 13.2 KiB, free: 912.3 MiB)
20/08/17 07:00:17 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1200
20/08/17 07:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[64] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:00:17 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
20/08/17 07:00:17 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:00:17 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)
20/08/17 07:00:17 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:00:17 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 2274 bytes result sent to driver
20/08/17 07:00:17 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 21 ms on localhost (executor driver) (1/1)
20/08/17 07:00:17 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
20/08/17 07:00:17 INFO DAGScheduler: ShuffleMapStage 13 (collect at utils.scala:116) finished in 0.037 s
20/08/17 07:00:17 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:00:17 INFO DAGScheduler: running: Set()
20/08/17 07:00:17 INFO DAGScheduler: waiting: Set(ResultStage 14)
20/08/17 07:00:17 INFO DAGScheduler: failed: Set()
20/08/17 07:00:17 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[67] at collect at utils.scala:116), which has no missing parents
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 10.1 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:00:17 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1200
20/08/17 07:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[67] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:00:17 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
20/08/17 07:00:17 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:00:17 INFO Executor: Running task 0.0 in stage 14.0 (TID 13)
20/08/17 07:00:17 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:00:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
20/08/17 07:00:17 INFO Executor: Finished task 0.0 in stage 14.0 (TID 13). 2648 bytes result sent to driver
20/08/17 07:00:17 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 17 ms on localhost (executor driver) (1/1)
20/08/17 07:00:17 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
20/08/17 07:00:17 INFO DAGScheduler: ResultStage 14 (collect at utils.scala:116) finished in 0.042 s
20/08/17 07:00:17 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:00:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
20/08/17 07:00:17 INFO DAGScheduler: Job 8 finished: collect at utils.scala:116, took 0.108743 s
20/08/17 07:00:17 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:00:17 INFO DAGScheduler: Registering RDD 72 (count at utils.scala:114) as input to shuffle 7
20/08/17 07:00:17 INFO DAGScheduler: Got job 9 (count at utils.scala:114) with 1 output partitions
20/08/17 07:00:17 INFO DAGScheduler: Final stage: ResultStage 16 (count at utils.scala:114)
20/08/17 07:00:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
20/08/17 07:00:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
20/08/17 07:00:17 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[72] at count at utils.scala:114), which has no missing parents
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 36.6 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:44647 (size: 13.0 KiB, free: 912.2 MiB)
20/08/17 07:00:17 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1200
20/08/17 07:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[72] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:00:17 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
20/08/17 07:00:17 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:00:17 INFO Executor: Running task 0.0 in stage 15.0 (TID 14)
20/08/17 07:00:17 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:00:17 INFO Executor: Finished task 0.0 in stage 15.0 (TID 14). 2274 bytes result sent to driver
20/08/17 07:00:17 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 14) in 23 ms on localhost (executor driver) (1/1)
20/08/17 07:00:17 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
20/08/17 07:00:17 INFO DAGScheduler: ShuffleMapStage 15 (count at utils.scala:114) finished in 0.054 s
20/08/17 07:00:17 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:00:17 INFO DAGScheduler: running: Set()
20/08/17 07:00:17 INFO DAGScheduler: waiting: Set(ResultStage 16)
20/08/17 07:00:17 INFO DAGScheduler: failed: Set()
20/08/17 07:00:17 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[75] at count at utils.scala:114), which has no missing parents
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 12.1 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.1 MiB)
20/08/17 07:00:17 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:00:17 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1200
20/08/17 07:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[75] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:00:17 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
20/08/17 07:00:17 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 15, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:00:17 INFO Executor: Running task 0.0 in stage 16.0 (TID 15)
20/08/17 07:00:17 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:00:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:00:17 INFO Executor: Finished task 0.0 in stage 16.0 (TID 15). 2896 bytes result sent to driver
20/08/17 07:00:17 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 15) in 22 ms on localhost (executor driver) (1/1)
20/08/17 07:00:17 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
20/08/17 07:00:17 INFO DAGScheduler: ResultStage 16 (count at utils.scala:114) finished in 0.056 s
20/08/17 07:00:17 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:00:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
20/08/17 07:00:17 INFO DAGScheduler: Job 9 finished: count at utils.scala:114, took 0.143718 s
20/08/17 07:01:45 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:01:45 INFO DAGScheduler: Registering RDD 80 (collect at utils.scala:116) as input to shuffle 8
20/08/17 07:01:45 INFO DAGScheduler: Got job 10 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:01:45 INFO DAGScheduler: Final stage: ResultStage 18 (collect at utils.scala:116)
20/08/17 07:01:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
20/08/17 07:01:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)
20/08/17 07:01:45 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[80] at collect at utils.scala:116), which has no missing parents
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 37.2 KiB, free 912.0 MiB)
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 912.0 MiB)
20/08/17 07:01:45 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:44647 (size: 13.2 KiB, free: 912.2 MiB)
20/08/17 07:01:45 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1200
20/08/17 07:01:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[80] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:01:45 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
20/08/17 07:01:45 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:01:45 INFO Executor: Running task 0.0 in stage 17.0 (TID 16)
20/08/17 07:01:45 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:01:45 INFO Executor: Finished task 0.0 in stage 17.0 (TID 16). 2274 bytes result sent to driver
20/08/17 07:01:45 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 16) in 29 ms on localhost (executor driver) (1/1)
20/08/17 07:01:45 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
20/08/17 07:01:45 INFO DAGScheduler: ShuffleMapStage 17 (collect at utils.scala:116) finished in 0.039 s
20/08/17 07:01:45 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:01:45 INFO DAGScheduler: running: Set()
20/08/17 07:01:45 INFO DAGScheduler: waiting: Set(ResultStage 18)
20/08/17 07:01:45 INFO DAGScheduler: failed: Set()
20/08/17 07:01:45 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[83] at collect at utils.scala:116), which has no missing parents
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 10.0 KiB, free 912.0 MiB)
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.0 MiB)
20/08/17 07:01:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:01:45 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1200
20/08/17 07:01:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[83] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:01:45 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
20/08/17 07:01:45 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 17, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:01:45 INFO Executor: Running task 0.0 in stage 18.0 (TID 17)
20/08/17 07:01:45 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:01:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:01:45 INFO Executor: Finished task 0.0 in stage 18.0 (TID 17). 2609 bytes result sent to driver
20/08/17 07:01:45 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 17) in 10 ms on localhost (executor driver) (1/1)
20/08/17 07:01:45 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
20/08/17 07:01:45 INFO DAGScheduler: ResultStage 18 (collect at utils.scala:116) finished in 0.020 s
20/08/17 07:01:45 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:01:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
20/08/17 07:01:45 INFO DAGScheduler: Job 10 finished: collect at utils.scala:116, took 0.073153 s
20/08/17 07:01:45 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:44647 in memory (size: 13.0 KiB, free: 912.2 MiB)
20/08/17 07:01:45 INFO CodeGenerator: Code generated in 28.93218 ms
20/08/17 07:01:45 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:01:45 INFO DAGScheduler: Registering RDD 88 (count at utils.scala:114) as input to shuffle 9
20/08/17 07:01:45 INFO DAGScheduler: Got job 11 (count at utils.scala:114) with 1 output partitions
20/08/17 07:01:45 INFO DAGScheduler: Final stage: ResultStage 20 (count at utils.scala:114)
20/08/17 07:01:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
20/08/17 07:01:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
20/08/17 07:01:45 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:44647 in memory (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:01:45 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[88] at count at utils.scala:114), which has no missing parents
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 36.6 KiB, free 912.0 MiB)
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 912.0 MiB)
20/08/17 07:01:45 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:44647 (size: 13.0 KiB, free: 912.2 MiB)
20/08/17 07:01:45 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1200
20/08/17 07:01:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[88] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:01:45 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
20/08/17 07:01:45 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:01:45 INFO Executor: Running task 0.0 in stage 19.0 (TID 18)
20/08/17 07:01:45 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:01:45 INFO Executor: Finished task 0.0 in stage 19.0 (TID 18). 2274 bytes result sent to driver
20/08/17 07:01:45 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 18) in 58 ms on localhost (executor driver) (1/1)
20/08/17 07:01:45 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
20/08/17 07:01:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:44647 in memory (size: 13.2 KiB, free: 912.2 MiB)
20/08/17 07:01:45 INFO DAGScheduler: ShuffleMapStage 19 (count at utils.scala:114) finished in 0.097 s
20/08/17 07:01:45 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:01:45 INFO DAGScheduler: running: Set()
20/08/17 07:01:45 INFO DAGScheduler: waiting: Set(ResultStage 20)
20/08/17 07:01:45 INFO DAGScheduler: failed: Set()
20/08/17 07:01:45 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[91] at count at utils.scala:114), which has no missing parents
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 12.4 KiB, free 912.1 MiB)
20/08/17 07:01:45 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 912.1 MiB)
20/08/17 07:01:45 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:44647 (size: 5.4 KiB, free: 912.2 MiB)
20/08/17 07:01:45 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1200
20/08/17 07:01:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[91] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:01:45 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
20/08/17 07:01:45 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 19, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:01:45 INFO Executor: Running task 0.0 in stage 20.0 (TID 19)
20/08/17 07:01:45 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:01:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:01:46 INFO Executor: Finished task 0.0 in stage 20.0 (TID 19). 2896 bytes result sent to driver
20/08/17 07:01:46 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 19) in 20 ms on localhost (executor driver) (1/1)
20/08/17 07:01:46 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
20/08/17 07:01:46 INFO DAGScheduler: ResultStage 20 (count at utils.scala:114) finished in 0.053 s
20/08/17 07:01:46 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:01:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
20/08/17 07:01:46 INFO DAGScheduler: Job 11 finished: count at utils.scala:114, took 0.193110 s
20/08/17 07:01:46 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:44647 in memory (size: 13.2 KiB, free: 912.2 MiB)
20/08/17 07:01:46 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 07:01:46 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 07:01:46 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 07:01:46 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 07:01:46 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:44647 in memory (size: 12.8 KiB, free: 912.3 MiB)
20/08/17 07:14:07 INFO CodeGenerator: Code generated in 18.330464 ms
20/08/17 07:14:07 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:14:07 INFO DAGScheduler: Got job 12 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:14:07 INFO DAGScheduler: Final stage: ResultStage 21 (collect at utils.scala:116)
20/08/17 07:14:07 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:14:07 INFO DAGScheduler: Missing parents: List()
20/08/17 07:14:07 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[96] at collect at utils.scala:116), which has no missing parents
20/08/17 07:14:07 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 35.9 KiB, free 912.2 MiB)
20/08/17 07:14:07 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 912.2 MiB)
20/08/17 07:14:07 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:44647 (size: 12.3 KiB, free: 912.3 MiB)
20/08/17 07:14:07 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1200
20/08/17 07:14:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[96] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:14:07 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
20/08/17 07:14:07 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 8649 bytes)
20/08/17 07:14:07 INFO Executor: Running task 0.0 in stage 21.0 (TID 20)
20/08/17 07:14:07 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:14:07 INFO Executor: 1 block locks were not released by TID = 20:
[rdd_16_0]
20/08/17 07:14:07 INFO Executor: Finished task 0.0 in stage 21.0 (TID 20). 1903 bytes result sent to driver
20/08/17 07:14:07 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 20) in 21 ms on localhost (executor driver) (1/1)
20/08/17 07:14:07 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
20/08/17 07:14:07 INFO DAGScheduler: ResultStage 21 (collect at utils.scala:116) finished in 0.033 s
20/08/17 07:14:07 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:14:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
20/08/17 07:14:07 INFO DAGScheduler: Job 12 finished: collect at utils.scala:116, took 0.040906 s
20/08/17 07:14:07 INFO CodeGenerator: Code generated in 17.039579 ms
20/08/17 07:14:08 INFO CodeGenerator: Code generated in 22.385106 ms
20/08/17 07:14:08 INFO CodeGenerator: Code generated in 28.085429 ms
20/08/17 07:14:08 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:14:08 INFO DAGScheduler: Registering RDD 101 (count at utils.scala:114) as input to shuffle 10
20/08/17 07:14:08 INFO DAGScheduler: Got job 13 (count at utils.scala:114) with 1 output partitions
20/08/17 07:14:08 INFO DAGScheduler: Final stage: ResultStage 23 (count at utils.scala:114)
20/08/17 07:14:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
20/08/17 07:14:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 22)
20/08/17 07:14:08 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[101] at count at utils.scala:114), which has no missing parents
20/08/17 07:14:08 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 36.1 KiB, free 912.1 MiB)
20/08/17 07:14:08 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.1 MiB)
20/08/17 07:14:08 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:44647 (size: 12.7 KiB, free: 912.3 MiB)
20/08/17 07:14:08 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1200
20/08/17 07:14:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[101] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:14:08 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
20/08/17 07:14:08 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:14:08 INFO Executor: Running task 0.0 in stage 22.0 (TID 21)
20/08/17 07:14:08 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:14:08 INFO Executor: 1 block locks were not released by TID = 21:
[rdd_16_0]
20/08/17 07:14:08 INFO Executor: Finished task 0.0 in stage 22.0 (TID 21). 2127 bytes result sent to driver
20/08/17 07:14:08 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 21) in 35 ms on localhost (executor driver) (1/1)
20/08/17 07:14:08 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
20/08/17 07:14:08 INFO DAGScheduler: ShuffleMapStage 22 (count at utils.scala:114) finished in 0.055 s
20/08/17 07:14:08 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:14:08 INFO DAGScheduler: running: Set()
20/08/17 07:14:08 INFO DAGScheduler: waiting: Set(ResultStage 23)
20/08/17 07:14:08 INFO DAGScheduler: failed: Set()
20/08/17 07:14:08 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[104] at count at utils.scala:114), which has no missing parents
20/08/17 07:14:08 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 11.5 KiB, free 912.1 MiB)
20/08/17 07:14:08 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.1 MiB)
20/08/17 07:14:08 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:14:08 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1200
20/08/17 07:14:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[104] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:14:08 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
20/08/17 07:14:08 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 22, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:14:08 INFO Executor: Running task 0.0 in stage 23.0 (TID 22)
20/08/17 07:14:08 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:14:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
20/08/17 07:14:08 INFO Executor: Finished task 0.0 in stage 23.0 (TID 22). 2471 bytes result sent to driver
20/08/17 07:14:08 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 22) in 26 ms on localhost (executor driver) (1/1)
20/08/17 07:14:08 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
20/08/17 07:14:08 INFO DAGScheduler: ResultStage 23 (count at utils.scala:114) finished in 0.039 s
20/08/17 07:14:08 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:14:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
20/08/17 07:14:08 INFO DAGScheduler: Job 13 finished: count at utils.scala:114, took 0.115560 s
20/08/17 07:15:39 INFO CodeGenerator: Code generated in 12.020667 ms
20/08/17 07:15:39 INFO CodeGenerator: Code generated in 18.446508 ms
20/08/17 07:15:39 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:15:39 INFO DAGScheduler: Registering RDD 109 (collect at utils.scala:116) as input to shuffle 11
20/08/17 07:15:39 INFO DAGScheduler: Got job 14 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:15:39 INFO DAGScheduler: Final stage: ResultStage 25 (collect at utils.scala:116)
20/08/17 07:15:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
20/08/17 07:15:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 24)
20/08/17 07:15:39 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[109] at collect at utils.scala:116), which has no missing parents
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 37.7 KiB, free 912.1 MiB)
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 912.1 MiB)
20/08/17 07:15:39 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:44647 (size: 13.1 KiB, free: 912.2 MiB)
20/08/17 07:15:39 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1200
20/08/17 07:15:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[109] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:15:39 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
20/08/17 07:15:39 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:15:39 INFO Executor: Running task 0.0 in stage 24.0 (TID 23)
20/08/17 07:15:39 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:15:39 INFO Executor: Finished task 0.0 in stage 24.0 (TID 23). 2127 bytes result sent to driver
20/08/17 07:15:39 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 23) in 56 ms on localhost (executor driver) (1/1)
20/08/17 07:15:39 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
20/08/17 07:15:39 INFO DAGScheduler: ShuffleMapStage 24 (collect at utils.scala:116) finished in 0.071 s
20/08/17 07:15:39 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:15:39 INFO DAGScheduler: running: Set()
20/08/17 07:15:39 INFO DAGScheduler: waiting: Set(ResultStage 25)
20/08/17 07:15:39 INFO DAGScheduler: failed: Set()
20/08/17 07:15:39 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[112] at collect at utils.scala:116), which has no missing parents
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 9.6 KiB, free 912.1 MiB)
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 912.1 MiB)
20/08/17 07:15:39 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:44647 (size: 4.7 KiB, free: 912.2 MiB)
20/08/17 07:15:39 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1200
20/08/17 07:15:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[112] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:15:39 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
20/08/17 07:15:39 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 24, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:15:39 INFO Executor: Running task 0.0 in stage 25.0 (TID 24)
20/08/17 07:15:39 INFO ShuffleBlockFetcherIterator: Getting 1 (490.0 B) non-empty blocks including 1 (490.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:15:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/08/17 07:15:39 INFO Executor: Finished task 0.0 in stage 25.0 (TID 24). 2240 bytes result sent to driver
20/08/17 07:15:39 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 24) in 40 ms on localhost (executor driver) (1/1)
20/08/17 07:15:39 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
20/08/17 07:15:39 INFO DAGScheduler: ResultStage 25 (collect at utils.scala:116) finished in 0.057 s
20/08/17 07:15:39 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:15:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
20/08/17 07:15:39 INFO DAGScheduler: Job 14 finished: collect at utils.scala:116, took 0.140476 s
20/08/17 07:15:39 INFO CodeGenerator: Code generated in 38.611099 ms
20/08/17 07:15:39 INFO CodeGenerator: Code generated in 62.350299 ms
20/08/17 07:15:39 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:15:39 INFO DAGScheduler: Registering RDD 117 (count at utils.scala:114) as input to shuffle 12
20/08/17 07:15:39 INFO DAGScheduler: Got job 15 (count at utils.scala:114) with 1 output partitions
20/08/17 07:15:39 INFO DAGScheduler: Final stage: ResultStage 27 (count at utils.scala:114)
20/08/17 07:15:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
20/08/17 07:15:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 26)
20/08/17 07:15:39 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[117] at count at utils.scala:114), which has no missing parents
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 36.1 KiB, free 912.0 MiB)
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.0 MiB)
20/08/17 07:15:39 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:44647 (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:15:39 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1200
20/08/17 07:15:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[117] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:15:39 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
20/08/17 07:15:39 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:15:39 INFO Executor: Running task 0.0 in stage 26.0 (TID 25)
20/08/17 07:15:39 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:15:39 INFO Executor: 1 block locks were not released by TID = 25:
[rdd_16_0]
20/08/17 07:15:39 INFO Executor: Finished task 0.0 in stage 26.0 (TID 25). 2127 bytes result sent to driver
20/08/17 07:15:39 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 25) in 23 ms on localhost (executor driver) (1/1)
20/08/17 07:15:39 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
20/08/17 07:15:39 INFO DAGScheduler: ShuffleMapStage 26 (count at utils.scala:114) finished in 0.053 s
20/08/17 07:15:39 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:15:39 INFO DAGScheduler: running: Set()
20/08/17 07:15:39 INFO DAGScheduler: waiting: Set(ResultStage 27)
20/08/17 07:15:39 INFO DAGScheduler: failed: Set()
20/08/17 07:15:39 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[120] at count at utils.scala:114), which has no missing parents
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.5 KiB, free 912.0 MiB)
20/08/17 07:15:39 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.0 MiB)
20/08/17 07:15:39 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:15:39 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1200
20/08/17 07:15:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[120] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:15:39 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
20/08/17 07:15:39 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:15:39 INFO Executor: Running task 0.0 in stage 27.0 (TID 26)
20/08/17 07:15:39 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:15:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
20/08/17 07:15:40 INFO Executor: Finished task 0.0 in stage 27.0 (TID 26). 2471 bytes result sent to driver
20/08/17 07:15:40 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 47 ms on localhost (executor driver) (1/1)
20/08/17 07:15:40 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
20/08/17 07:15:40 INFO DAGScheduler: ResultStage 27 (count at utils.scala:114) finished in 0.066 s
20/08/17 07:15:40 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:15:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
20/08/17 07:15:40 INFO DAGScheduler: Job 15 finished: count at utils.scala:114, took 0.134751 s
20/08/17 07:16:42 INFO CodeGenerator: Code generated in 15.742952 ms
20/08/17 07:16:42 INFO CodeGenerator: Code generated in 21.705925 ms
20/08/17 07:16:42 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:16:42 INFO DAGScheduler: Registering RDD 125 (collect at utils.scala:116) as input to shuffle 13
20/08/17 07:16:42 INFO DAGScheduler: Got job 16 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:16:42 INFO DAGScheduler: Final stage: ResultStage 29 (collect at utils.scala:116)
20/08/17 07:16:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
20/08/17 07:16:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)
20/08/17 07:16:42 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[125] at collect at utils.scala:116), which has no missing parents
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 37.7 KiB, free 912.0 MiB)
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 911.9 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:44647 (size: 13.1 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1200
20/08/17 07:16:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[125] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:16:42 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
20/08/17 07:16:42 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:16:42 INFO Executor: Running task 0.0 in stage 28.0 (TID 27)
20/08/17 07:16:42 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:16:42 INFO Executor: Finished task 0.0 in stage 28.0 (TID 27). 2127 bytes result sent to driver
20/08/17 07:16:42 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 27) in 15 ms on localhost (executor driver) (1/1)
20/08/17 07:16:42 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
20/08/17 07:16:42 INFO DAGScheduler: ShuffleMapStage 28 (collect at utils.scala:116) finished in 0.023 s
20/08/17 07:16:42 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:16:42 INFO DAGScheduler: running: Set()
20/08/17 07:16:42 INFO DAGScheduler: waiting: Set(ResultStage 29)
20/08/17 07:16:42 INFO DAGScheduler: failed: Set()
20/08/17 07:16:42 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[128] at collect at utils.scala:116), which has no missing parents
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 9.7 KiB, free 911.9 MiB)
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 911.9 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:44647 (size: 4.7 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1200
20/08/17 07:16:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[128] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:16:42 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
20/08/17 07:16:42 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 28, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:16:42 INFO Executor: Running task 0.0 in stage 29.0 (TID 28)
20/08/17 07:16:42 INFO ShuffleBlockFetcherIterator: Getting 1 (490.0 B) non-empty blocks including 1 (490.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:16:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:16:42 INFO Executor: Finished task 0.0 in stage 29.0 (TID 28). 2768 bytes result sent to driver
20/08/17 07:16:42 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 28) in 10 ms on localhost (executor driver) (1/1)
20/08/17 07:16:42 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
20/08/17 07:16:42 INFO DAGScheduler: ResultStage 29 (collect at utils.scala:116) finished in 0.018 s
20/08/17 07:16:42 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:16:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
20/08/17 07:16:42 INFO DAGScheduler: Job 16 finished: collect at utils.scala:116, took 0.048822 s
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:44647 in memory (size: 4.7 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO CodeGenerator: Code generated in 58.238453 ms
20/08/17 07:16:42 INFO CodeGenerator: Code generated in 42.822008 ms
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:44647 in memory (size: 13.1 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:16:42 INFO DAGScheduler: Registering RDD 133 (count at utils.scala:114) as input to shuffle 14
20/08/17 07:16:42 INFO DAGScheduler: Got job 17 (count at utils.scala:114) with 1 output partitions
20/08/17 07:16:42 INFO DAGScheduler: Final stage: ResultStage 31 (count at utils.scala:114)
20/08/17 07:16:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)
20/08/17 07:16:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 30)
20/08/17 07:16:42 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[133] at count at utils.scala:114), which has no missing parents
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 36.1 KiB, free 912.0 MiB)
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.0 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:44647 (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1200
20/08/17 07:16:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[133] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:16:42 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
20/08/17 07:16:42 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:16:42 INFO Executor: Running task 0.0 in stage 30.0 (TID 29)
20/08/17 07:16:42 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:44647 in memory (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO Executor: Finished task 0.0 in stage 30.0 (TID 29). 2127 bytes result sent to driver
20/08/17 07:16:42 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 29) in 50 ms on localhost (executor driver) (1/1)
20/08/17 07:16:42 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
20/08/17 07:16:42 INFO DAGScheduler: ShuffleMapStage 30 (count at utils.scala:114) finished in 0.086 s
20/08/17 07:16:42 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:16:42 INFO DAGScheduler: running: Set()
20/08/17 07:16:42 INFO DAGScheduler: waiting: Set(ResultStage 31)
20/08/17 07:16:42 INFO DAGScheduler: failed: Set()
20/08/17 07:16:42 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[136] at count at utils.scala:114), which has no missing parents
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 11.5 KiB, free 912.0 MiB)
20/08/17 07:16:42 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.0 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1200
20/08/17 07:16:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[136] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:16:42 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
20/08/17 07:16:42 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 30, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:16:42 INFO Executor: Running task 0.0 in stage 31.0 (TID 30)
20/08/17 07:16:42 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:16:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:16:42 INFO Executor: Finished task 0.0 in stage 31.0 (TID 30). 2772 bytes result sent to driver
20/08/17 07:16:42 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 30) in 39 ms on localhost (executor driver) (1/1)
20/08/17 07:16:42 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
20/08/17 07:16:42 INFO DAGScheduler: ResultStage 31 (count at utils.scala:114) finished in 0.055 s
20/08/17 07:16:42 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:16:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
20/08/17 07:16:42 INFO DAGScheduler: Job 17 finished: count at utils.scala:114, took 0.168776 s
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:44647 in memory (size: 13.1 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:44647 in memory (size: 4.7 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:44647 in memory (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:44647 in memory (size: 5.4 KiB, free: 912.3 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:44647 in memory (size: 13.0 KiB, free: 912.3 MiB)
20/08/17 07:16:42 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:44647 in memory (size: 12.3 KiB, free: 912.3 MiB)
20/08/17 07:17:05 INFO CodeGenerator: Code generated in 17.179194 ms
20/08/17 07:17:05 INFO CodeGenerator: Code generated in 23.373147 ms
20/08/17 07:17:05 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:17:05 INFO DAGScheduler: Registering RDD 141 (collect at utils.scala:116) as input to shuffle 15
20/08/17 07:17:05 INFO DAGScheduler: Got job 18 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:17:05 INFO DAGScheduler: Final stage: ResultStage 33 (collect at utils.scala:116)
20/08/17 07:17:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
20/08/17 07:17:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 32)
20/08/17 07:17:05 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[141] at collect at utils.scala:116), which has no missing parents
20/08/17 07:17:05 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 37.7 KiB, free 912.2 MiB)
20/08/17 07:17:05 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 912.2 MiB)
20/08/17 07:17:05 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:44647 (size: 13.2 KiB, free: 912.3 MiB)
20/08/17 07:17:05 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1200
20/08/17 07:17:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[141] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:17:05 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
20/08/17 07:17:05 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:17:05 INFO Executor: Running task 0.0 in stage 32.0 (TID 31)
20/08/17 07:17:05 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:17:05 INFO Executor: Finished task 0.0 in stage 32.0 (TID 31). 2127 bytes result sent to driver
20/08/17 07:17:05 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 31) in 35 ms on localhost (executor driver) (1/1)
20/08/17 07:17:05 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
20/08/17 07:17:05 INFO DAGScheduler: ShuffleMapStage 32 (collect at utils.scala:116) finished in 0.046 s
20/08/17 07:17:05 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:17:05 INFO DAGScheduler: running: Set()
20/08/17 07:17:05 INFO DAGScheduler: waiting: Set(ResultStage 33)
20/08/17 07:17:05 INFO DAGScheduler: failed: Set()
20/08/17 07:17:05 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[144] at collect at utils.scala:116), which has no missing parents
20/08/17 07:17:05 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 9.7 KiB, free 912.2 MiB)
20/08/17 07:17:05 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 912.2 MiB)
20/08/17 07:17:05 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:44647 (size: 4.7 KiB, free: 912.3 MiB)
20/08/17 07:17:05 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1200
20/08/17 07:17:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[144] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:17:05 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
20/08/17 07:17:05 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 32, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:17:05 INFO Executor: Running task 0.0 in stage 33.0 (TID 32)
20/08/17 07:17:06 INFO ShuffleBlockFetcherIterator: Getting 1 (490.0 B) non-empty blocks including 1 (490.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:17:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:17:06 INFO Executor: Finished task 0.0 in stage 33.0 (TID 32). 2768 bytes result sent to driver
20/08/17 07:17:06 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 32) in 14 ms on localhost (executor driver) (1/1)
20/08/17 07:17:06 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
20/08/17 07:17:06 INFO DAGScheduler: ResultStage 33 (collect at utils.scala:116) finished in 0.022 s
20/08/17 07:17:06 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:17:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
20/08/17 07:17:06 INFO DAGScheduler: Job 18 finished: collect at utils.scala:116, took 0.079235 s
20/08/17 07:17:06 INFO CodeGenerator: Code generated in 24.198312 ms
20/08/17 07:17:06 INFO CodeGenerator: Code generated in 10.954038 ms
20/08/17 07:17:06 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:17:06 INFO DAGScheduler: Registering RDD 149 (count at utils.scala:114) as input to shuffle 16
20/08/17 07:17:06 INFO DAGScheduler: Got job 19 (count at utils.scala:114) with 1 output partitions
20/08/17 07:17:06 INFO DAGScheduler: Final stage: ResultStage 35 (count at utils.scala:114)
20/08/17 07:17:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
20/08/17 07:17:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 34)
20/08/17 07:17:06 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[149] at count at utils.scala:114), which has no missing parents
20/08/17 07:17:06 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 36.1 KiB, free 912.1 MiB)
20/08/17 07:17:06 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.1 MiB)
20/08/17 07:17:06 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:44647 (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:17:06 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1200
20/08/17 07:17:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[149] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:17:06 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
20/08/17 07:17:06 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:17:06 INFO Executor: Running task 0.0 in stage 34.0 (TID 33)
20/08/17 07:17:06 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:17:06 INFO Executor: Finished task 0.0 in stage 34.0 (TID 33). 2127 bytes result sent to driver
20/08/17 07:17:06 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 33) in 46 ms on localhost (executor driver) (1/1)
20/08/17 07:17:06 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
20/08/17 07:17:06 INFO DAGScheduler: ShuffleMapStage 34 (count at utils.scala:114) finished in 0.056 s
20/08/17 07:17:06 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:17:06 INFO DAGScheduler: running: Set()
20/08/17 07:17:06 INFO DAGScheduler: waiting: Set(ResultStage 35)
20/08/17 07:17:06 INFO DAGScheduler: failed: Set()
20/08/17 07:17:06 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[152] at count at utils.scala:114), which has no missing parents
20/08/17 07:17:06 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 11.5 KiB, free 912.1 MiB)
20/08/17 07:17:06 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.1 MiB)
20/08/17 07:17:06 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:17:06 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1200
20/08/17 07:17:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[152] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:17:06 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
20/08/17 07:17:06 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 34, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:17:06 INFO Executor: Running task 0.0 in stage 35.0 (TID 34)
20/08/17 07:17:06 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:17:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:17:06 INFO Executor: Finished task 0.0 in stage 35.0 (TID 34). 2772 bytes result sent to driver
20/08/17 07:17:06 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 34) in 28 ms on localhost (executor driver) (1/1)
20/08/17 07:17:06 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
20/08/17 07:17:06 INFO DAGScheduler: ResultStage 35 (count at utils.scala:114) finished in 0.056 s
20/08/17 07:17:06 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:17:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
20/08/17 07:17:06 INFO DAGScheduler: Job 19 finished: count at utils.scala:114, took 0.139073 s
20/08/17 07:27:02 INFO Instrumentation: [24874f55] training finished
20/08/17 07:27:03 INFO Instrumentation: [4c747bf4] training finished
20/08/17 07:27:03 ERROR Instrumentation: java.lang.IllegalArgumentException: label does not exist. Available: mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, features
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.MapLike.getOrElse(MapLike.scala:131)
	at scala.collection.MapLike.getOrElse$(MapLike.scala:129)
	at scala.collection.AbstractMap.getOrElse(Map.scala:63)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:75)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:53)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:46)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:119)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:176)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:132)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableViewLike$Transformed.foreach(IterableViewLike.scala:47)
	at scala.collection.IterableViewLike$Transformed.foreach$(IterableViewLike.scala:47)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:40)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:147)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:136)
	at sparklyr.StreamHandler.read(stream.scala:61)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:39)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

20/08/17 07:27:32 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:27:32 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:44647 in memory (size: 12.7 KiB, free: 912.3 MiB)
20/08/17 07:27:32 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:44647 in memory (size: 13.2 KiB, free: 912.3 MiB)
20/08/17 07:27:32 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:44647 in memory (size: 4.7 KiB, free: 912.3 MiB)
20/08/17 07:27:32 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 07:27:32 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:44647 in memory (size: 12.7 KiB, free: 912.3 MiB)
20/08/17 07:27:43 INFO Instrumentation: [873eec40] training finished
20/08/17 07:27:43 INFO Instrumentation: [7eb5d5e6] training finished
20/08/17 07:27:43 ERROR Instrumentation: java.lang.IllegalArgumentException: label does not exist. Available: mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, features
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.MapLike.getOrElse(MapLike.scala:131)
	at scala.collection.MapLike.getOrElse$(MapLike.scala:129)
	at scala.collection.AbstractMap.getOrElse(Map.scala:63)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:75)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:53)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:46)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:119)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:176)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:132)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableViewLike$Transformed.foreach(IterableViewLike.scala:47)
	at scala.collection.IterableViewLike$Transformed.foreach$(IterableViewLike.scala:47)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:40)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:147)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:136)
	at sparklyr.StreamHandler.read(stream.scala:61)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:39)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

20/08/17 07:29:07 INFO Instrumentation: [5186f159] training finished
20/08/17 07:29:07 INFO Instrumentation: [72b6df8b] training finished
20/08/17 07:29:07 ERROR Instrumentation: java.lang.IllegalArgumentException: label does not exist. Available: mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, features
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.MapLike.getOrElse(MapLike.scala:131)
	at scala.collection.MapLike.getOrElse$(MapLike.scala:129)
	at scala.collection.AbstractMap.getOrElse(Map.scala:63)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:75)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:53)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:46)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:119)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:176)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:132)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableViewLike$Transformed.foreach(IterableViewLike.scala:47)
	at scala.collection.IterableViewLike$Transformed.foreach$(IterableViewLike.scala:47)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:40)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:147)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:136)
	at sparklyr.StreamHandler.read(stream.scala:61)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:39)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

20/08/17 07:29:45 INFO CodeGenerator: Code generated in 22.067728 ms
20/08/17 07:29:45 INFO CodeGenerator: Code generated in 20.485299 ms
20/08/17 07:29:46 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:29:46 INFO DAGScheduler: Registering RDD 157 (collect at utils.scala:116) as input to shuffle 17
20/08/17 07:29:46 INFO DAGScheduler: Got job 20 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:29:46 INFO DAGScheduler: Final stage: ResultStage 37 (collect at utils.scala:116)
20/08/17 07:29:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)
20/08/17 07:29:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 36)
20/08/17 07:29:46 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[157] at collect at utils.scala:116), which has no missing parents
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 37.7 KiB, free 912.3 MiB)
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 912.2 MiB)
20/08/17 07:29:46 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:44647 (size: 13.1 KiB, free: 912.3 MiB)
20/08/17 07:29:46 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1200
20/08/17 07:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[157] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:29:46 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
20/08/17 07:29:46 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:29:46 INFO Executor: Running task 0.0 in stage 36.0 (TID 35)
20/08/17 07:29:46 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:29:46 INFO Executor: Finished task 0.0 in stage 36.0 (TID 35). 2127 bytes result sent to driver
20/08/17 07:29:46 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 35) in 22 ms on localhost (executor driver) (1/1)
20/08/17 07:29:46 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
20/08/17 07:29:46 INFO DAGScheduler: ShuffleMapStage 36 (collect at utils.scala:116) finished in 0.033 s
20/08/17 07:29:46 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:29:46 INFO DAGScheduler: running: Set()
20/08/17 07:29:46 INFO DAGScheduler: waiting: Set(ResultStage 37)
20/08/17 07:29:46 INFO DAGScheduler: failed: Set()
20/08/17 07:29:46 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[160] at collect at utils.scala:116), which has no missing parents
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 9.7 KiB, free 912.2 MiB)
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 912.2 MiB)
20/08/17 07:29:46 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:44647 (size: 4.7 KiB, free: 912.3 MiB)
20/08/17 07:29:46 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1200
20/08/17 07:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[160] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:29:46 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
20/08/17 07:29:46 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 36, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:29:46 INFO Executor: Running task 0.0 in stage 37.0 (TID 36)
20/08/17 07:29:46 INFO ShuffleBlockFetcherIterator: Getting 1 (490.0 B) non-empty blocks including 1 (490.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:29:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:29:46 INFO Executor: Finished task 0.0 in stage 37.0 (TID 36). 2768 bytes result sent to driver
20/08/17 07:29:46 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 36) in 11 ms on localhost (executor driver) (1/1)
20/08/17 07:29:46 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
20/08/17 07:29:46 INFO DAGScheduler: ResultStage 37 (collect at utils.scala:116) finished in 0.019 s
20/08/17 07:29:46 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:29:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
20/08/17 07:29:46 INFO DAGScheduler: Job 20 finished: collect at utils.scala:116, took 0.064350 s
20/08/17 07:29:46 INFO CodeGenerator: Code generated in 20.681644 ms
20/08/17 07:29:46 INFO CodeGenerator: Code generated in 14.50469 ms
20/08/17 07:29:46 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:29:46 INFO DAGScheduler: Registering RDD 165 (count at utils.scala:114) as input to shuffle 18
20/08/17 07:29:46 INFO DAGScheduler: Got job 21 (count at utils.scala:114) with 1 output partitions
20/08/17 07:29:46 INFO DAGScheduler: Final stage: ResultStage 39 (count at utils.scala:114)
20/08/17 07:29:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
20/08/17 07:29:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 38)
20/08/17 07:29:46 INFO DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[165] at count at utils.scala:114), which has no missing parents
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 36.1 KiB, free 912.2 MiB)
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
20/08/17 07:29:46 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on localhost:44647 (size: 12.7 KiB, free: 912.3 MiB)
20/08/17 07:29:46 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1200
20/08/17 07:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[165] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:29:46 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
20/08/17 07:29:46 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:29:46 INFO Executor: Running task 0.0 in stage 38.0 (TID 37)
20/08/17 07:29:46 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:29:46 INFO Executor: Finished task 0.0 in stage 38.0 (TID 37). 2127 bytes result sent to driver
20/08/17 07:29:46 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 37) in 15 ms on localhost (executor driver) (1/1)
20/08/17 07:29:46 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
20/08/17 07:29:46 INFO DAGScheduler: ShuffleMapStage 38 (count at utils.scala:114) finished in 0.022 s
20/08/17 07:29:46 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:29:46 INFO DAGScheduler: running: Set()
20/08/17 07:29:46 INFO DAGScheduler: waiting: Set(ResultStage 39)
20/08/17 07:29:46 INFO DAGScheduler: failed: Set()
20/08/17 07:29:46 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[168] at count at utils.scala:114), which has no missing parents
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 11.5 KiB, free 912.2 MiB)
20/08/17 07:29:46 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.2 MiB)
20/08/17 07:29:46 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 07:29:46 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1200
20/08/17 07:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[168] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:29:46 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
20/08/17 07:29:46 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 38, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:29:46 INFO Executor: Running task 0.0 in stage 39.0 (TID 38)
20/08/17 07:29:46 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:29:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:29:46 INFO Executor: Finished task 0.0 in stage 39.0 (TID 38). 2772 bytes result sent to driver
20/08/17 07:29:46 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 38) in 19 ms on localhost (executor driver) (1/1)
20/08/17 07:29:46 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
20/08/17 07:29:46 INFO DAGScheduler: ResultStage 39 (count at utils.scala:114) finished in 0.036 s
20/08/17 07:29:46 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:29:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
20/08/17 07:29:46 INFO DAGScheduler: Job 21 finished: count at utils.scala:114, took 0.066191 s
20/08/17 07:33:28 INFO CodeGenerator: Code generated in 23.636863 ms
20/08/17 07:33:28 INFO CodeGenerator: Code generated in 18.07564 ms
20/08/17 07:33:28 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:33:28 INFO DAGScheduler: Registering RDD 173 (collect at utils.scala:116) as input to shuffle 19
20/08/17 07:33:28 INFO DAGScheduler: Got job 22 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:33:28 INFO DAGScheduler: Final stage: ResultStage 41 (collect at utils.scala:116)
20/08/17 07:33:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)
20/08/17 07:33:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 40)
20/08/17 07:33:28 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[173] at collect at utils.scala:116), which has no missing parents
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 37.7 KiB, free 912.1 MiB)
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 912.1 MiB)
20/08/17 07:33:28 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on localhost:44647 (size: 13.1 KiB, free: 912.2 MiB)
20/08/17 07:33:28 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1200
20/08/17 07:33:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[173] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:33:28 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
20/08/17 07:33:28 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:33:28 INFO Executor: Running task 0.0 in stage 40.0 (TID 39)
20/08/17 07:33:28 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:33:28 INFO Executor: Finished task 0.0 in stage 40.0 (TID 39). 2127 bytes result sent to driver
20/08/17 07:33:28 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 39) in 46 ms on localhost (executor driver) (1/1)
20/08/17 07:33:28 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
20/08/17 07:33:28 INFO DAGScheduler: ShuffleMapStage 40 (collect at utils.scala:116) finished in 0.056 s
20/08/17 07:33:28 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:33:28 INFO DAGScheduler: running: Set()
20/08/17 07:33:28 INFO DAGScheduler: waiting: Set(ResultStage 41)
20/08/17 07:33:28 INFO DAGScheduler: failed: Set()
20/08/17 07:33:28 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[176] at collect at utils.scala:116), which has no missing parents
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 9.7 KiB, free 912.1 MiB)
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 912.1 MiB)
20/08/17 07:33:28 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on localhost:44647 (size: 4.7 KiB, free: 912.2 MiB)
20/08/17 07:33:28 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1200
20/08/17 07:33:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[176] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:33:28 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
20/08/17 07:33:28 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 40, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:33:28 INFO Executor: Running task 0.0 in stage 41.0 (TID 40)
20/08/17 07:33:28 INFO ShuffleBlockFetcherIterator: Getting 1 (490.0 B) non-empty blocks including 1 (490.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:33:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:33:28 INFO Executor: Finished task 0.0 in stage 41.0 (TID 40). 2768 bytes result sent to driver
20/08/17 07:33:28 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 40) in 10 ms on localhost (executor driver) (1/1)
20/08/17 07:33:28 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
20/08/17 07:33:28 INFO DAGScheduler: ResultStage 41 (collect at utils.scala:116) finished in 0.027 s
20/08/17 07:33:28 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:33:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
20/08/17 07:33:28 INFO DAGScheduler: Job 22 finished: collect at utils.scala:116, took 0.094826 s
20/08/17 07:33:28 INFO CodeGenerator: Code generated in 34.958297 ms
20/08/17 07:33:28 INFO CodeGenerator: Code generated in 10.457434 ms
20/08/17 07:33:28 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:33:28 INFO DAGScheduler: Registering RDD 181 (count at utils.scala:114) as input to shuffle 20
20/08/17 07:33:28 INFO DAGScheduler: Got job 23 (count at utils.scala:114) with 1 output partitions
20/08/17 07:33:28 INFO DAGScheduler: Final stage: ResultStage 43 (count at utils.scala:114)
20/08/17 07:33:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)
20/08/17 07:33:28 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 42)
20/08/17 07:33:28 INFO DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[181] at count at utils.scala:114), which has no missing parents
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 36.1 KiB, free 912.1 MiB)
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.1 MiB)
20/08/17 07:33:28 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on localhost:44647 (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:33:28 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1200
20/08/17 07:33:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[181] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:33:28 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
20/08/17 07:33:28 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:33:28 INFO Executor: Running task 0.0 in stage 42.0 (TID 41)
20/08/17 07:33:28 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:33:28 INFO Executor: Finished task 0.0 in stage 42.0 (TID 41). 2127 bytes result sent to driver
20/08/17 07:33:28 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 41) in 36 ms on localhost (executor driver) (1/1)
20/08/17 07:33:28 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
20/08/17 07:33:28 INFO DAGScheduler: ShuffleMapStage 42 (count at utils.scala:114) finished in 0.051 s
20/08/17 07:33:28 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:33:28 INFO DAGScheduler: running: Set()
20/08/17 07:33:28 INFO DAGScheduler: waiting: Set(ResultStage 43)
20/08/17 07:33:28 INFO DAGScheduler: failed: Set()
20/08/17 07:33:28 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[184] at count at utils.scala:114), which has no missing parents
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 11.5 KiB, free 912.0 MiB)
20/08/17 07:33:28 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.0 MiB)
20/08/17 07:33:28 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:33:28 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1200
20/08/17 07:33:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[184] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:33:28 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks
20/08/17 07:33:28 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 42, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:33:28 INFO Executor: Running task 0.0 in stage 43.0 (TID 42)
20/08/17 07:33:28 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:33:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:33:28 INFO Executor: Finished task 0.0 in stage 43.0 (TID 42). 2772 bytes result sent to driver
20/08/17 07:33:28 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 42) in 26 ms on localhost (executor driver) (1/1)
20/08/17 07:33:28 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
20/08/17 07:33:28 INFO DAGScheduler: ResultStage 43 (count at utils.scala:114) finished in 0.050 s
20/08/17 07:33:28 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:33:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
20/08/17 07:33:28 INFO DAGScheduler: Job 23 finished: count at utils.scala:114, took 0.130063 s
20/08/17 07:33:43 INFO Instrumentation: [2c1b0431] training finished
20/08/17 07:33:43 INFO Instrumentation: [3352ee04] training finished
20/08/17 07:33:43 ERROR Instrumentation: java.lang.IllegalArgumentException: label does not exist. Available: mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, features
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.MapLike.getOrElse(MapLike.scala:131)
	at scala.collection.MapLike.getOrElse$(MapLike.scala:129)
	at scala.collection.AbstractMap.getOrElse(Map.scala:63)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:75)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:53)
	at org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:46)
	at org.apache.spark.ml.regression.LinearRegression.org$apache$spark$ml$regression$LinearRegressionParams$$super$validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema(LinearRegression.scala:119)
	at org.apache.spark.ml.regression.LinearRegressionParams.validateAndTransformSchema$(LinearRegression.scala:107)
	at org.apache.spark.ml.regression.LinearRegression.validateAndTransformSchema(LinearRegression.scala:176)
	at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:176)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:132)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableViewLike$Transformed.foreach(IterableViewLike.scala:47)
	at scala.collection.IterableViewLike$Transformed.foreach$(IterableViewLike.scala:47)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:40)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:147)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:136)
	at sparklyr.StreamHandler.read(stream.scala:61)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:39)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

20/08/17 07:34:07 INFO Instrumentation: [03699ccf] training finished
20/08/17 07:34:07 INFO Instrumentation: [800bc8ce] training finished
20/08/17 07:34:07 INFO CodeGenerator: Code generated in 21.474731 ms
20/08/17 07:34:07 INFO Instrumentation: [4b0157d0] Stage class: LinearRegression
20/08/17 07:34:07 INFO Instrumentation: [4b0157d0] Stage uid: linear_regression__96d0ebcc_d11d_445e_8aef_bf8752e67fd5
20/08/17 07:34:07 INFO BlockManagerInfo: Removed broadcast_37_piece0 on localhost:44647 in memory (size: 12.7 KiB, free: 912.2 MiB)
20/08/17 07:34:07 INFO BlockManagerInfo: Removed broadcast_38_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:34:07 INFO BlockManagerInfo: Removed broadcast_39_piece0 on localhost:44647 in memory (size: 13.1 KiB, free: 912.3 MiB)
20/08/17 07:34:07 INFO BlockManagerInfo: Removed broadcast_42_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 07:34:07 INFO CodeGenerator: Code generated in 156.900414 ms
20/08/17 07:34:07 INFO BlockManagerInfo: Removed broadcast_36_piece0 on localhost:44647 in memory (size: 4.7 KiB, free: 912.3 MiB)
20/08/17 07:34:07 INFO Instrumentation: [4b0157d0] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)
20/08/17 07:34:07 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:44647 in memory (size: 13.1 KiB, free: 912.3 MiB)
20/08/17 07:34:08 INFO BlockManagerInfo: Removed broadcast_40_piece0 on localhost:44647 in memory (size: 4.7 KiB, free: 912.3 MiB)
20/08/17 07:34:08 INFO BlockManagerInfo: Removed broadcast_41_piece0 on localhost:44647 in memory (size: 12.7 KiB, free: 912.3 MiB)
20/08/17 07:34:08 INFO Instrumentation: [4b0157d0] {"elasticNetParam":0.0,"featuresCol":"features","fitIntercept":true,"solver":"auto","labelCol":"label","predictionCol":"prediction","standardization":true,"loss":"squaredError","regParam":0.0,"tol":1.0E-6,"maxIter":100}
20/08/17 07:34:08 INFO Instrumentation: [4b0157d0] {"numFeatures":1}
20/08/17 07:34:08 WARN Instrumentation: [4b0157d0] regParam is zero, which might cause numerical instability and overfitting.
20/08/17 07:34:08 INFO SparkContext: Starting job: treeAggregate at WeightedLeastSquares.scala:107
20/08/17 07:34:08 INFO DAGScheduler: Got job 24 (treeAggregate at WeightedLeastSquares.scala:107) with 1 output partitions
20/08/17 07:34:08 INFO DAGScheduler: Final stage: ResultStage 44 (treeAggregate at WeightedLeastSquares.scala:107)
20/08/17 07:34:08 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:34:08 INFO DAGScheduler: Missing parents: List()
20/08/17 07:34:08 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[200] at treeAggregate at WeightedLeastSquares.scala:107), which has no missing parents
20/08/17 07:34:08 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 54.3 KiB, free 912.2 MiB)
20/08/17 07:34:08 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 912.2 MiB)
20/08/17 07:34:08 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on localhost:44647 (size: 19.1 KiB, free: 912.3 MiB)
20/08/17 07:34:08 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1200
20/08/17 07:34:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[200] at treeAggregate at WeightedLeastSquares.scala:107) (first 15 tasks are for partitions Vector(0))
20/08/17 07:34:08 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks
20/08/17 07:34:08 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 43, localhost, executor driver, partition 0, PROCESS_LOCAL, 8649 bytes)
20/08/17 07:34:08 INFO Executor: Running task 0.0 in stage 44.0 (TID 43)
20/08/17 07:34:08 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:34:08 INFO CodeGenerator: Code generated in 21.038477 ms
20/08/17 07:34:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
20/08/17 07:34:08 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
20/08/17 07:34:08 INFO Executor: Finished task 0.0 in stage 44.0 (TID 43). 2032 bytes result sent to driver
20/08/17 07:34:08 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 43) in 379 ms on localhost (executor driver) (1/1)
20/08/17 07:34:08 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
20/08/17 07:34:08 INFO DAGScheduler: ResultStage 44 (treeAggregate at WeightedLeastSquares.scala:107) finished in 0.462 s
20/08/17 07:34:08 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:34:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
20/08/17 07:34:08 INFO DAGScheduler: Job 24 finished: treeAggregate at WeightedLeastSquares.scala:107, took 0.486557 s
20/08/17 07:34:08 INFO Instrumentation: [4b0157d0] Number of instances: 32.
20/08/17 07:34:08 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
20/08/17 07:34:08 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
20/08/17 07:34:09 INFO CodeGenerator: Code generated in 52.664523 ms
20/08/17 07:34:09 INFO SparkContext: Starting job: treeAggregate at Statistics.scala:58
20/08/17 07:34:09 INFO DAGScheduler: Got job 25 (treeAggregate at Statistics.scala:58) with 1 output partitions
20/08/17 07:34:09 INFO DAGScheduler: Final stage: ResultStage 45 (treeAggregate at Statistics.scala:58)
20/08/17 07:34:09 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:34:09 INFO DAGScheduler: Missing parents: List()
20/08/17 07:34:09 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[210] at treeAggregate at Statistics.scala:58), which has no missing parents
20/08/17 07:34:09 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 57.3 KiB, free 912.2 MiB)
20/08/17 07:34:09 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 912.1 MiB)
20/08/17 07:34:09 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on localhost:44647 (size: 21.0 KiB, free: 912.3 MiB)
20/08/17 07:34:09 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1200
20/08/17 07:34:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[210] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))
20/08/17 07:34:09 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
20/08/17 07:34:09 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 8649 bytes)
20/08/17 07:34:09 INFO Executor: Running task 0.0 in stage 45.0 (TID 44)
20/08/17 07:34:09 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:34:09 INFO CodeGenerator: Code generated in 22.937247 ms
20/08/17 07:34:09 INFO Executor: Finished task 0.0 in stage 45.0 (TID 44). 2877 bytes result sent to driver
20/08/17 07:34:09 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 44) in 164 ms on localhost (executor driver) (1/1)
20/08/17 07:34:09 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
20/08/17 07:34:09 INFO DAGScheduler: ResultStage 45 (treeAggregate at Statistics.scala:58) finished in 0.185 s
20/08/17 07:34:09 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:34:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
20/08/17 07:34:09 INFO DAGScheduler: Job 25 finished: treeAggregate at Statistics.scala:58, took 0.198174 s
20/08/17 07:34:09 INFO SparkContext: Starting job: count at LinearRegression.scala:933
20/08/17 07:34:09 INFO DAGScheduler: Registering RDD 215 (count at LinearRegression.scala:933) as input to shuffle 21
20/08/17 07:34:09 INFO DAGScheduler: Got job 26 (count at LinearRegression.scala:933) with 1 output partitions
20/08/17 07:34:09 INFO DAGScheduler: Final stage: ResultStage 47 (count at LinearRegression.scala:933)
20/08/17 07:34:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
20/08/17 07:34:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 46)
20/08/17 07:34:09 INFO DAGScheduler: Submitting ShuffleMapStage 46 (MapPartitionsRDD[215] at count at LinearRegression.scala:933), which has no missing parents
20/08/17 07:34:09 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 37.4 KiB, free 912.1 MiB)
20/08/17 07:34:09 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 13.3 KiB, free 912.1 MiB)
20/08/17 07:34:09 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on localhost:44647 (size: 13.3 KiB, free: 912.2 MiB)
20/08/17 07:34:09 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1200
20/08/17 07:34:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[215] at count at LinearRegression.scala:933) (first 15 tasks are for partitions Vector(0))
20/08/17 07:34:09 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks
20/08/17 07:34:09 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 8638 bytes)
20/08/17 07:34:09 INFO Executor: Running task 0.0 in stage 46.0 (TID 45)
20/08/17 07:34:09 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:34:09 INFO Executor: Finished task 0.0 in stage 46.0 (TID 45). 2274 bytes result sent to driver
20/08/17 07:34:09 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 45) in 9 ms on localhost (executor driver) (1/1)
20/08/17 07:34:09 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
20/08/17 07:34:09 INFO DAGScheduler: ShuffleMapStage 46 (count at LinearRegression.scala:933) finished in 0.027 s
20/08/17 07:34:09 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:34:09 INFO DAGScheduler: running: Set()
20/08/17 07:34:09 INFO DAGScheduler: waiting: Set(ResultStage 47)
20/08/17 07:34:09 INFO DAGScheduler: failed: Set()
20/08/17 07:34:09 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[218] at count at LinearRegression.scala:933), which has no missing parents
20/08/17 07:34:09 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 10.1 KiB, free 912.1 MiB)
20/08/17 07:34:09 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.1 MiB)
20/08/17 07:34:09 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:34:09 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1200
20/08/17 07:34:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[218] at count at LinearRegression.scala:933) (first 15 tasks are for partitions Vector(0))
20/08/17 07:34:09 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
20/08/17 07:34:09 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 46, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:34:09 INFO Executor: Running task 0.0 in stage 47.0 (TID 46)
20/08/17 07:34:09 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:34:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:34:09 INFO Executor: Finished task 0.0 in stage 47.0 (TID 46). 2648 bytes result sent to driver
20/08/17 07:34:09 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 46) in 12 ms on localhost (executor driver) (1/1)
20/08/17 07:34:09 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
20/08/17 07:34:09 INFO DAGScheduler: ResultStage 47 (count at LinearRegression.scala:933) finished in 0.023 s
20/08/17 07:34:09 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:34:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
20/08/17 07:34:09 INFO DAGScheduler: Job 26 finished: count at LinearRegression.scala:933, took 0.060803 s
20/08/17 07:34:09 INFO Instrumentation: [0ecbc6a5] training finished
20/08/17 07:34:11 INFO Instrumentation: [8528c91f] training finished
20/08/17 07:49:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/08/17 07:49:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/08/17 07:49:33 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/08/17 07:49:33 INFO DAGScheduler: Got job 27 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 07:49:33 INFO DAGScheduler: Final stage: ResultStage 48 (csv at NativeMethodAccessorImpl.java:0)
20/08/17 07:49:33 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:49:33 INFO DAGScheduler: Missing parents: List()
20/08/17 07:49:33 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[222] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 07:49:33 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 184.6 KiB, free 911.9 MiB)
20/08/17 07:49:33 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 63.5 KiB, free 911.8 MiB)
20/08/17 07:49:33 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on localhost:44647 (size: 63.5 KiB, free: 912.2 MiB)
20/08/17 07:49:33 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1200
20/08/17 07:49:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[222] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 07:49:33 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks
20/08/17 07:49:33 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 47, localhost, executor driver, partition 0, PROCESS_LOCAL, 8649 bytes)
20/08/17 07:49:33 INFO Executor: Running task 0.0 in stage 48.0 (TID 47)
20/08/17 07:49:34 INFO BlockManager: Found block rdd_16_0 locally
20/08/17 07:49:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
20/08/17 07:49:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20/08/17 07:49:34 INFO FileOutputCommitter: Saved output of task 'attempt_20200817074933_0048_m_000000_47' to file:/home/emmanuel/R/Dissertation/car.csv/_temporary/0/task_20200817074933_0048_m_000000
20/08/17 07:49:34 INFO SparkHadoopMapRedUtil: attempt_20200817074933_0048_m_000000_47: Committed
20/08/17 07:49:34 INFO Executor: Finished task 0.0 in stage 48.0 (TID 47). 2668 bytes result sent to driver
20/08/17 07:49:34 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 47) in 697 ms on localhost (executor driver) (1/1)
20/08/17 07:49:34 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
20/08/17 07:49:34 INFO DAGScheduler: ResultStage 48 (csv at NativeMethodAccessorImpl.java:0) finished in 0.737 s
20/08/17 07:49:34 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:49:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
20/08/17 07:49:34 INFO DAGScheduler: Job 27 finished: csv at NativeMethodAccessorImpl.java:0, took 0.750716 s
20/08/17 07:49:34 INFO FileFormatWriter: Write Job a24a4069-8799-4435-aa16-efd10eb9eef7 committed.
20/08/17 07:49:34 INFO FileFormatWriter: Finished processing stats for write job a24a4069-8799-4435-aa16-efd10eb9eef7.
20/08/17 07:52:06 INFO HiveMetaStore: 0: get_database: default
20/08/17 07:52:06 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 07:52:06 INFO HiveMetaStore: 0: get_database: default
20/08/17 07:52:06 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 07:52:06 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 07:52:06 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 07:52:07 INFO BlockManagerInfo: Removed broadcast_46_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:52:07 INFO CodeGenerator: Code generated in 17.934112 ms
20/08/17 07:52:07 INFO BlockManagerInfo: Removed broadcast_47_piece0 on localhost:44647 in memory (size: 63.5 KiB, free: 912.2 MiB)
20/08/17 07:52:07 INFO BlockManagerInfo: Removed broadcast_44_piece0 on localhost:44647 in memory (size: 21.0 KiB, free: 912.3 MiB)
20/08/17 07:52:07 INFO SparkContext: Starting job: collect at utils.scala:41
20/08/17 07:52:07 INFO DAGScheduler: Got job 28 (collect at utils.scala:41) with 2 output partitions
20/08/17 07:52:07 INFO DAGScheduler: Final stage: ResultStage 49 (collect at utils.scala:41)
20/08/17 07:52:07 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:52:07 INFO DAGScheduler: Missing parents: List()
20/08/17 07:52:07 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[230] at map at utils.scala:41), which has no missing parents
20/08/17 07:52:07 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 8.5 KiB, free 912.2 MiB)
20/08/17 07:52:07 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 912.2 MiB)
20/08/17 07:52:07 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on localhost:44647 (size: 4.5 KiB, free: 912.3 MiB)
20/08/17 07:52:07 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 49 (MapPartitionsRDD[230] at map at utils.scala:41) (first 15 tasks are for partitions Vector(0, 1))
20/08/17 07:52:07 INFO TaskSchedulerImpl: Adding task set 49.0 with 2 tasks
20/08/17 07:52:07 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 7622 bytes)
20/08/17 07:52:07 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 49, localhost, executor driver, partition 1, PROCESS_LOCAL, 7670 bytes)
20/08/17 07:52:07 INFO Executor: Running task 0.0 in stage 49.0 (TID 48)
20/08/17 07:52:07 INFO Executor: Running task 1.0 in stage 49.0 (TID 49)
20/08/17 07:52:07 INFO BlockManagerInfo: Removed broadcast_43_piece0 on localhost:44647 in memory (size: 19.1 KiB, free: 912.3 MiB)
20/08/17 07:52:07 INFO CodeGenerator: Code generated in 24.18437 ms
20/08/17 07:52:07 INFO Executor: Finished task 1.0 in stage 49.0 (TID 49). 1207 bytes result sent to driver
20/08/17 07:52:07 INFO Executor: Finished task 0.0 in stage 49.0 (TID 48). 1164 bytes result sent to driver
20/08/17 07:52:07 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 49) in 71 ms on localhost (executor driver) (1/2)
20/08/17 07:52:07 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 48) in 73 ms on localhost (executor driver) (2/2)
20/08/17 07:52:07 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
20/08/17 07:52:07 INFO DAGScheduler: ResultStage 49 (collect at utils.scala:41) finished in 0.082 s
20/08/17 07:52:07 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
20/08/17 07:52:07 INFO DAGScheduler: Job 28 finished: collect at utils.scala:41, took 0.094556 s
20/08/17 07:52:07 INFO BlockManagerInfo: Removed broadcast_45_piece0 on localhost:44647 in memory (size: 13.3 KiB, free: 912.3 MiB)
20/08/17 07:52:08 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
20/08/17 07:52:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 07:52:08 INFO FileSourceStrategy: Pruning directories with: 
20/08/17 07:52:08 INFO FileSourceStrategy: Pushed Filters: 
20/08/17 07:52:08 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#5375, None)) > 0)
20/08/17 07:52:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
20/08/17 07:52:08 INFO CodeGenerator: Code generated in 18.538282 ms
20/08/17 07:52:08 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 292.8 KiB, free 912.0 MiB)
20/08/17 07:52:08 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 912.0 MiB)
20/08/17 07:52:08 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on localhost:44647 (size: 24.4 KiB, free: 912.3 MiB)
20/08/17 07:52:08 INFO SparkContext: Created broadcast 49 from csv at NativeMethodAccessorImpl.java:0
20/08/17 07:52:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/08/17 07:52:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/08/17 07:52:08 INFO DAGScheduler: Got job 29 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 07:52:08 INFO DAGScheduler: Final stage: ResultStage 50 (csv at NativeMethodAccessorImpl.java:0)
20/08/17 07:52:08 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:52:08 INFO DAGScheduler: Missing parents: List()
20/08/17 07:52:08 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[234] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 07:52:08 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 10.7 KiB, free 912.0 MiB)
20/08/17 07:52:08 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 912.0 MiB)
20/08/17 07:52:08 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 07:52:08 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[234] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:08 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks
20/08/17 07:52:08 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50, localhost, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)
20/08/17 07:52:08 INFO Executor: Running task 0.0 in stage 50.0 (TID 50)
20/08/17 07:52:08 INFO FileScanRDD: Reading File path: file:///home/emmanuel/R/Dissertation/car.csv/part-00000-dc3c4ec6-19be-4bf6-be86-a2bf846645ad-c000.csv, range: 0-1717, partition values: [empty row]
20/08/17 07:52:08 INFO CodeGenerator: Code generated in 10.943328 ms
20/08/17 07:52:08 INFO Executor: Finished task 0.0 in stage 50.0 (TID 50). 1564 bytes result sent to driver
20/08/17 07:52:08 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 191 ms on localhost (executor driver) (1/1)
20/08/17 07:52:08 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
20/08/17 07:52:08 INFO DAGScheduler: ResultStage 50 (csv at NativeMethodAccessorImpl.java:0) finished in 0.229 s
20/08/17 07:52:08 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
20/08/17 07:52:08 INFO DAGScheduler: Job 29 finished: csv at NativeMethodAccessorImpl.java:0, took 0.247995 s
20/08/17 07:52:08 INFO CodeGenerator: Code generated in 15.231978 ms
20/08/17 07:52:09 INFO FileSourceStrategy: Pruning directories with: 
20/08/17 07:52:09 INFO FileSourceStrategy: Pushed Filters: 
20/08/17 07:52:09 INFO FileSourceStrategy: Post-Scan Filters: 
20/08/17 07:52:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
20/08/17 07:52:09 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 292.8 KiB, free 911.7 MiB)
20/08/17 07:52:09 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 911.6 MiB)
20/08/17 07:52:09 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on localhost:44647 (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 07:52:09 INFO SparkContext: Created broadcast 51 from csv at NativeMethodAccessorImpl.java:0
20/08/17 07:52:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/08/17 07:52:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/08/17 07:52:09 INFO DAGScheduler: Got job 30 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 07:52:09 INFO DAGScheduler: Final stage: ResultStage 51 (csv at NativeMethodAccessorImpl.java:0)
20/08/17 07:52:09 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:52:09 INFO DAGScheduler: Missing parents: List()
20/08/17 07:52:09 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[240] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 07:52:09 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 15.5 KiB, free 911.6 MiB)
20/08/17 07:52:09 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 911.6 MiB)
20/08/17 07:52:09 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on localhost:44647 (size: 7.8 KiB, free: 912.2 MiB)
20/08/17 07:52:09 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[240] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:09 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks
20/08/17 07:52:09 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)
20/08/17 07:52:09 INFO Executor: Running task 0.0 in stage 51.0 (TID 51)
20/08/17 07:52:09 INFO FileScanRDD: Reading File path: file:///home/emmanuel/R/Dissertation/car.csv/part-00000-dc3c4ec6-19be-4bf6-be86-a2bf846645ad-c000.csv, range: 0-1717, partition values: [empty row]
20/08/17 07:52:09 INFO Executor: Finished task 0.0 in stage 51.0 (TID 51). 1640 bytes result sent to driver
20/08/17 07:52:09 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 90 ms on localhost (executor driver) (1/1)
20/08/17 07:52:09 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
20/08/17 07:52:09 INFO DAGScheduler: ResultStage 51 (csv at NativeMethodAccessorImpl.java:0) finished in 0.131 s
20/08/17 07:52:09 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
20/08/17 07:52:09 INFO DAGScheduler: Job 30 finished: csv at NativeMethodAccessorImpl.java:0, took 0.140050 s
20/08/17 07:52:10 INFO HiveMetaStore: 0: get_database: default
20/08/17 07:52:10 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 07:52:10 INFO HiveMetaStore: 0: get_database: default
20/08/17 07:52:10 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 07:52:10 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 07:52:10 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 07:52:10 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:52:10 INFO DAGScheduler: Registering RDD 243 (count at utils.scala:114) as input to shuffle 22
20/08/17 07:52:10 INFO DAGScheduler: Got job 31 (count at utils.scala:114) with 1 output partitions
20/08/17 07:52:10 INFO DAGScheduler: Final stage: ResultStage 53 (count at utils.scala:114)
20/08/17 07:52:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)
20/08/17 07:52:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 52)
20/08/17 07:52:10 INFO DAGScheduler: Submitting ShuffleMapStage 52 (MapPartitionsRDD[243] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:10 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 10.1 KiB, free 911.6 MiB)
20/08/17 07:52:10 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 911.6 MiB)
20/08/17 07:52:10 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:52:10 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[243] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0, 1))
20/08/17 07:52:10 INFO TaskSchedulerImpl: Adding task set 52.0 with 2 tasks
20/08/17 07:52:10 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52, localhost, executor driver, partition 0, PROCESS_LOCAL, 7515 bytes)
20/08/17 07:52:10 INFO TaskSetManager: Starting task 1.0 in stage 52.0 (TID 53, localhost, executor driver, partition 1, PROCESS_LOCAL, 7515 bytes)
20/08/17 07:52:10 INFO Executor: Running task 0.0 in stage 52.0 (TID 52)
20/08/17 07:52:10 INFO Executor: Finished task 0.0 in stage 52.0 (TID 52). 1833 bytes result sent to driver
20/08/17 07:52:10 INFO Executor: Running task 1.0 in stage 52.0 (TID 53)
20/08/17 07:52:10 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 5 ms on localhost (executor driver) (1/2)
20/08/17 07:52:10 INFO Executor: Finished task 1.0 in stage 52.0 (TID 53). 1833 bytes result sent to driver
20/08/17 07:52:10 INFO TaskSetManager: Finished task 1.0 in stage 52.0 (TID 53) in 10 ms on localhost (executor driver) (2/2)
20/08/17 07:52:10 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
20/08/17 07:52:10 INFO DAGScheduler: ShuffleMapStage 52 (count at utils.scala:114) finished in 0.021 s
20/08/17 07:52:10 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:52:10 INFO DAGScheduler: running: Set()
20/08/17 07:52:10 INFO DAGScheduler: waiting: Set(ResultStage 53)
20/08/17 07:52:10 INFO DAGScheduler: failed: Set()
20/08/17 07:52:10 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[246] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:10 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 10.1 KiB, free 911.6 MiB)
20/08/17 07:52:10 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 911.6 MiB)
20/08/17 07:52:10 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:52:10 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[246] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:10 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks
20/08/17 07:52:10 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 54, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:52:10 INFO Executor: Running task 0.0 in stage 53.0 (TID 54)
20/08/17 07:52:10 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:52:10 INFO Executor: Finished task 0.0 in stage 53.0 (TID 54). 2648 bytes result sent to driver
20/08/17 07:52:10 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 54) in 6 ms on localhost (executor driver) (1/1)
20/08/17 07:52:10 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
20/08/17 07:52:10 INFO DAGScheduler: ResultStage 53 (count at utils.scala:114) finished in 0.011 s
20/08/17 07:52:10 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
20/08/17 07:52:10 INFO DAGScheduler: Job 31 finished: count at utils.scala:114, took 0.038620 s
20/08/17 07:52:11 INFO FileSourceStrategy: Pruning directories with: 
20/08/17 07:52:11 INFO FileSourceStrategy: Pushed Filters: 
20/08/17 07:52:11 INFO FileSourceStrategy: Post-Scan Filters: 
20/08/17 07:52:11 INFO FileSourceStrategy: Output Data Schema: struct<mpg: double, cyl: double, disp: double, hp: double, drat: double ... 9 more fields>
20/08/17 07:52:11 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 292.8 KiB, free 911.3 MiB)
20/08/17 07:52:11 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 911.3 MiB)
20/08/17 07:52:11 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on localhost:44647 (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 07:52:11 INFO SparkContext: Created broadcast 55 from sql at <unknown>:0
20/08/17 07:52:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/08/17 07:52:11 INFO SparkContext: Starting job: sql at <unknown>:0
20/08/17 07:52:11 INFO DAGScheduler: Registering RDD 254 (sql at <unknown>:0) as input to shuffle 23
20/08/17 07:52:11 INFO DAGScheduler: Got job 32 (sql at <unknown>:0) with 1 output partitions
20/08/17 07:52:11 INFO DAGScheduler: Final stage: ResultStage 55 (sql at <unknown>:0)
20/08/17 07:52:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)
20/08/17 07:52:11 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 54)
20/08/17 07:52:11 INFO DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[254] at sql at <unknown>:0), which has no missing parents
20/08/17 07:52:11 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 27.3 KiB, free 911.3 MiB)
20/08/17 07:52:11 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 911.2 MiB)
20/08/17 07:52:11 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on localhost:44647 (size: 11.8 KiB, free: 912.2 MiB)
20/08/17 07:52:11 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[254] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:11 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks
20/08/17 07:52:11 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 55, localhost, executor driver, partition 0, PROCESS_LOCAL, 7787 bytes)
20/08/17 07:52:11 INFO Executor: Running task 0.0 in stage 54.0 (TID 55)
20/08/17 07:52:11 INFO FileScanRDD: Reading File path: file:///home/emmanuel/R/Dissertation/car.csv/part-00000-dc3c4ec6-19be-4bf6-be86-a2bf846645ad-c000.csv, range: 0-1717, partition values: [empty row]
20/08/17 07:52:11 INFO CodeGenerator: Code generated in 16.78185 ms
20/08/17 07:52:11 INFO MemoryStore: Block rdd_249_0 stored as values in memory (estimated size 4.2 KiB, free 911.2 MiB)
20/08/17 07:52:11 INFO BlockManagerInfo: Added rdd_249_0 in memory on localhost:44647 (size: 4.2 KiB, free: 912.2 MiB)
20/08/17 07:52:11 INFO Executor: Finished task 0.0 in stage 54.0 (TID 55). 2212 bytes result sent to driver
20/08/17 07:52:11 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 55) in 74 ms on localhost (executor driver) (1/1)
20/08/17 07:52:11 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
20/08/17 07:52:11 INFO DAGScheduler: ShuffleMapStage 54 (sql at <unknown>:0) finished in 0.083 s
20/08/17 07:52:11 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:52:11 INFO DAGScheduler: running: Set()
20/08/17 07:52:11 INFO DAGScheduler: waiting: Set(ResultStage 55)
20/08/17 07:52:11 INFO DAGScheduler: failed: Set()
20/08/17 07:52:11 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[257] at sql at <unknown>:0), which has no missing parents
20/08/17 07:52:11 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 10.1 KiB, free 911.2 MiB)
20/08/17 07:52:11 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 911.2 MiB)
20/08/17 07:52:11 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:52:11 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[257] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:11 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks
20/08/17 07:52:11 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 56, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:52:11 INFO Executor: Running task 0.0 in stage 55.0 (TID 56)
20/08/17 07:52:11 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:52:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:52:11 INFO Executor: Finished task 0.0 in stage 55.0 (TID 56). 2648 bytes result sent to driver
20/08/17 07:52:11 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 56) in 24 ms on localhost (executor driver) (1/1)
20/08/17 07:52:11 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
20/08/17 07:52:11 INFO DAGScheduler: ResultStage 55 (sql at <unknown>:0) finished in 0.030 s
20/08/17 07:52:11 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished
20/08/17 07:52:11 INFO DAGScheduler: Job 32 finished: sql at <unknown>:0, took 0.120404 s
20/08/17 07:52:12 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:52:12 INFO DAGScheduler: Registering RDD 262 (collect at utils.scala:116) as input to shuffle 24
20/08/17 07:52:12 INFO DAGScheduler: Got job 33 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:52:12 INFO DAGScheduler: Final stage: ResultStage 57 (collect at utils.scala:116)
20/08/17 07:52:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 56)
20/08/17 07:52:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 56)
20/08/17 07:52:12 INFO DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[262] at collect at utils.scala:116), which has no missing parents
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 27.3 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 11.9 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on localhost:44647 (size: 11.9 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[262] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:12 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks
20/08/17 07:52:12 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 57, localhost, executor driver, partition 0, PROCESS_LOCAL, 7787 bytes)
20/08/17 07:52:12 INFO Executor: Running task 0.0 in stage 56.0 (TID 57)
20/08/17 07:52:12 INFO BlockManager: Found block rdd_249_0 locally
20/08/17 07:52:12 INFO Executor: Finished task 0.0 in stage 56.0 (TID 57). 2212 bytes result sent to driver
20/08/17 07:52:12 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 57) in 10 ms on localhost (executor driver) (1/1)
20/08/17 07:52:12 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
20/08/17 07:52:12 INFO DAGScheduler: ShuffleMapStage 56 (collect at utils.scala:116) finished in 0.021 s
20/08/17 07:52:12 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:52:12 INFO DAGScheduler: running: Set()
20/08/17 07:52:12 INFO DAGScheduler: waiting: Set(ResultStage 57)
20/08/17 07:52:12 INFO DAGScheduler: failed: Set()
20/08/17 07:52:12 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[265] at collect at utils.scala:116), which has no missing parents
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 10.1 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[265] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:12 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks
20/08/17 07:52:12 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 58, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:52:12 INFO Executor: Running task 0.0 in stage 57.0 (TID 58)
20/08/17 07:52:12 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:52:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:52:12 INFO Executor: Finished task 0.0 in stage 57.0 (TID 58). 2648 bytes result sent to driver
20/08/17 07:52:12 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 58) in 7 ms on localhost (executor driver) (1/1)
20/08/17 07:52:12 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
20/08/17 07:52:12 INFO DAGScheduler: ResultStage 57 (collect at utils.scala:116) finished in 0.012 s
20/08/17 07:52:12 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
20/08/17 07:52:12 INFO DAGScheduler: Job 33 finished: collect at utils.scala:116, took 0.041200 s
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_52_piece0 on localhost:44647 in memory (size: 7.8 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_57_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:52:12 INFO DAGScheduler: Registering RDD 270 (count at utils.scala:114) as input to shuffle 25
20/08/17 07:52:12 INFO DAGScheduler: Got job 34 (count at utils.scala:114) with 1 output partitions
20/08/17 07:52:12 INFO DAGScheduler: Final stage: ResultStage 59 (count at utils.scala:114)
20/08/17 07:52:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)
20/08/17 07:52:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 58)
20/08/17 07:52:12 INFO DAGScheduler: Submitting ShuffleMapStage 58 (MapPartitionsRDD[270] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 26.7 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on localhost:44647 (size: 11.6 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[270] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:12 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks
20/08/17 07:52:12 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 59, localhost, executor driver, partition 0, PROCESS_LOCAL, 7787 bytes)
20/08/17 07:52:12 INFO Executor: Running task 0.0 in stage 58.0 (TID 59)
20/08/17 07:52:12 INFO BlockManager: Found block rdd_249_0 locally
20/08/17 07:52:12 INFO Executor: Finished task 0.0 in stage 58.0 (TID 59). 2212 bytes result sent to driver
20/08/17 07:52:12 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 59) in 13 ms on localhost (executor driver) (1/1)
20/08/17 07:52:12 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
20/08/17 07:52:12 INFO DAGScheduler: ShuffleMapStage 58 (count at utils.scala:114) finished in 0.030 s
20/08/17 07:52:12 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:52:12 INFO DAGScheduler: running: Set()
20/08/17 07:52:12 INFO DAGScheduler: waiting: Set(ResultStage 59)
20/08/17 07:52:12 INFO DAGScheduler: failed: Set()
20/08/17 07:52:12 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[273] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 12.1 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_59_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 911.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[273] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:12 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks
20/08/17 07:52:12 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 60, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:52:12 INFO Executor: Running task 0.0 in stage 59.0 (TID 60)
20/08/17 07:52:12 INFO ShuffleBlockFetcherIterator: Getting 1 (49.0 B) non-empty blocks including 1 (49.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:52:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_58_piece0 on localhost:44647 in memory (size: 11.9 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO Executor: Finished task 0.0 in stage 59.0 (TID 60). 2896 bytes result sent to driver
20/08/17 07:52:12 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 60) in 7 ms on localhost (executor driver) (1/1)
20/08/17 07:52:12 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
20/08/17 07:52:12 INFO DAGScheduler: ResultStage 59 (count at utils.scala:114) finished in 0.021 s
20/08/17 07:52:12 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
20/08/17 07:52:12 INFO DAGScheduler: Job 34 finished: count at utils.scala:114, took 0.058928 s
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_50_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_53_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_51_piece0 on localhost:44647 in memory (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_48_piece0 on localhost:44647 in memory (size: 4.5 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_49_piece0 on localhost:44647 in memory (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_56_piece0 on localhost:44647 in memory (size: 11.8 KiB, free: 912.2 MiB)
20/08/17 07:52:12 INFO BlockManagerInfo: Removed broadcast_54_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 07:52:13 INFO HiveMetaStore: 0: get_database: default
20/08/17 07:52:13 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 07:52:13 INFO HiveMetaStore: 0: get_database: default
20/08/17 07:52:13 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 07:52:13 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 07:52:13 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 07:52:14 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:52:14 INFO DAGScheduler: Registering RDD 276 (count at utils.scala:114) as input to shuffle 26
20/08/17 07:52:14 INFO DAGScheduler: Got job 35 (count at utils.scala:114) with 1 output partitions
20/08/17 07:52:14 INFO DAGScheduler: Final stage: ResultStage 61 (count at utils.scala:114)
20/08/17 07:52:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 60)
20/08/17 07:52:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 60)
20/08/17 07:52:14 INFO DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[276] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:14 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 10.1 KiB, free 911.9 MiB)
20/08/17 07:52:14 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 911.9 MiB)
20/08/17 07:52:14 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:52:14 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:14 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[276] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0, 1))
20/08/17 07:52:14 INFO TaskSchedulerImpl: Adding task set 60.0 with 2 tasks
20/08/17 07:52:14 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 61, localhost, executor driver, partition 0, PROCESS_LOCAL, 7515 bytes)
20/08/17 07:52:14 INFO TaskSetManager: Starting task 1.0 in stage 60.0 (TID 62, localhost, executor driver, partition 1, PROCESS_LOCAL, 7532 bytes)
20/08/17 07:52:14 INFO Executor: Running task 1.0 in stage 60.0 (TID 62)
20/08/17 07:52:14 INFO Executor: Finished task 1.0 in stage 60.0 (TID 62). 1833 bytes result sent to driver
20/08/17 07:52:14 INFO Executor: Running task 0.0 in stage 60.0 (TID 61)
20/08/17 07:52:14 INFO TaskSetManager: Finished task 1.0 in stage 60.0 (TID 62) in 10 ms on localhost (executor driver) (1/2)
20/08/17 07:52:14 INFO Executor: Finished task 0.0 in stage 60.0 (TID 61). 1833 bytes result sent to driver
20/08/17 07:52:14 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 61) in 27 ms on localhost (executor driver) (2/2)
20/08/17 07:52:14 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
20/08/17 07:52:14 INFO DAGScheduler: ShuffleMapStage 60 (count at utils.scala:114) finished in 0.039 s
20/08/17 07:52:14 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:52:14 INFO DAGScheduler: running: Set()
20/08/17 07:52:14 INFO DAGScheduler: waiting: Set(ResultStage 61)
20/08/17 07:52:14 INFO DAGScheduler: failed: Set()
20/08/17 07:52:14 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[279] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:14 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 10.1 KiB, free 911.9 MiB)
20/08/17 07:52:14 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 911.9 MiB)
20/08/17 07:52:14 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on localhost:44647 (size: 5.0 KiB, free: 912.2 MiB)
20/08/17 07:52:14 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[279] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:14 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks
20/08/17 07:52:14 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 63, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:52:14 INFO Executor: Running task 0.0 in stage 61.0 (TID 63)
20/08/17 07:52:14 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:52:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:52:14 INFO Executor: Finished task 0.0 in stage 61.0 (TID 63). 2648 bytes result sent to driver
20/08/17 07:52:14 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 63) in 5 ms on localhost (executor driver) (1/1)
20/08/17 07:52:14 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
20/08/17 07:52:14 INFO DAGScheduler: ResultStage 61 (count at utils.scala:114) finished in 0.021 s
20/08/17 07:52:14 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
20/08/17 07:52:14 INFO DAGScheduler: Job 35 finished: count at utils.scala:114, took 0.065677 s
20/08/17 07:52:35 INFO SparkContext: Starting job: collect at utils.scala:116
20/08/17 07:52:35 INFO DAGScheduler: Got job 36 (collect at utils.scala:116) with 1 output partitions
20/08/17 07:52:35 INFO DAGScheduler: Final stage: ResultStage 62 (collect at utils.scala:116)
20/08/17 07:52:35 INFO DAGScheduler: Parents of final stage: List()
20/08/17 07:52:35 INFO DAGScheduler: Missing parents: List()
20/08/17 07:52:35 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[284] at collect at utils.scala:116), which has no missing parents
20/08/17 07:52:35 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 30.8 KiB, free 911.9 MiB)
20/08/17 07:52:35 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 911.9 MiB)
20/08/17 07:52:35 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on localhost:44647 (size: 11.5 KiB, free: 912.2 MiB)
20/08/17 07:52:35 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[284] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:35 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks
20/08/17 07:52:35 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 7798 bytes)
20/08/17 07:52:35 INFO Executor: Running task 0.0 in stage 62.0 (TID 64)
20/08/17 07:52:35 INFO BlockManager: Found block rdd_249_0 locally
20/08/17 07:52:35 INFO Executor: 1 block locks were not released by TID = 64:
[rdd_249_0]
20/08/17 07:52:35 INFO Executor: Finished task 0.0 in stage 62.0 (TID 64). 2207 bytes result sent to driver
20/08/17 07:52:35 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 64) in 10 ms on localhost (executor driver) (1/1)
20/08/17 07:52:35 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
20/08/17 07:52:35 INFO DAGScheduler: ResultStage 62 (collect at utils.scala:116) finished in 0.016 s
20/08/17 07:52:35 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
20/08/17 07:52:35 INFO DAGScheduler: Job 36 finished: collect at utils.scala:116, took 0.021253 s
20/08/17 07:52:35 INFO CodeGenerator: Code generated in 31.416201 ms
20/08/17 07:52:35 INFO CodeGenerator: Code generated in 14.16033 ms
20/08/17 07:52:35 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 07:52:35 INFO DAGScheduler: Registering RDD 289 (count at utils.scala:114) as input to shuffle 27
20/08/17 07:52:35 INFO DAGScheduler: Got job 37 (count at utils.scala:114) with 1 output partitions
20/08/17 07:52:35 INFO DAGScheduler: Final stage: ResultStage 64 (count at utils.scala:114)
20/08/17 07:52:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)
20/08/17 07:52:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 63)
20/08/17 07:52:35 INFO DAGScheduler: Submitting ShuffleMapStage 63 (MapPartitionsRDD[289] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:35 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 26.1 KiB, free 911.8 MiB)
20/08/17 07:52:35 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 11.4 KiB, free 911.8 MiB)
20/08/17 07:52:35 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on localhost:44647 (size: 11.4 KiB, free: 912.2 MiB)
20/08/17 07:52:35 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[289] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:35 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks
20/08/17 07:52:35 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 65, localhost, executor driver, partition 0, PROCESS_LOCAL, 7787 bytes)
20/08/17 07:52:35 INFO Executor: Running task 0.0 in stage 63.0 (TID 65)
20/08/17 07:52:35 INFO BlockManager: Found block rdd_249_0 locally
20/08/17 07:52:35 INFO Executor: 1 block locks were not released by TID = 65:
[rdd_249_0]
20/08/17 07:52:35 INFO Executor: Finished task 0.0 in stage 63.0 (TID 65). 2065 bytes result sent to driver
20/08/17 07:52:35 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 65) in 16 ms on localhost (executor driver) (1/1)
20/08/17 07:52:35 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
20/08/17 07:52:35 INFO DAGScheduler: ShuffleMapStage 63 (count at utils.scala:114) finished in 0.027 s
20/08/17 07:52:35 INFO DAGScheduler: looking for newly runnable stages
20/08/17 07:52:35 INFO DAGScheduler: running: Set()
20/08/17 07:52:35 INFO DAGScheduler: waiting: Set(ResultStage 64)
20/08/17 07:52:35 INFO DAGScheduler: failed: Set()
20/08/17 07:52:35 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[292] at count at utils.scala:114), which has no missing parents
20/08/17 07:52:35 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 11.5 KiB, free 911.8 MiB)
20/08/17 07:52:35 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 911.8 MiB)
20/08/17 07:52:35 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:52:35 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1200
20/08/17 07:52:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[292] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 07:52:35 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks
20/08/17 07:52:35 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 66, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/08/17 07:52:35 INFO Executor: Running task 0.0 in stage 64.0 (TID 66)
20/08/17 07:52:35 INFO ShuffleBlockFetcherIterator: Getting 1 (54.0 B) non-empty blocks including 1 (54.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 07:52:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/08/17 07:52:35 INFO Executor: Finished task 0.0 in stage 64.0 (TID 66). 2471 bytes result sent to driver
20/08/17 07:52:35 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 66) in 13 ms on localhost (executor driver) (1/1)
20/08/17 07:52:35 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
20/08/17 07:52:35 INFO DAGScheduler: ResultStage 64 (count at utils.scala:114) finished in 0.027 s
20/08/17 07:52:35 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 07:52:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
20/08/17 07:52:35 INFO DAGScheduler: Job 37 finished: count at utils.scala:114, took 0.066451 s
20/08/17 07:57:31 INFO BlockManagerInfo: Removed broadcast_64_piece0 on localhost:44647 in memory (size: 11.5 KiB, free: 912.2 MiB)
20/08/17 07:57:31 INFO BlockManagerInfo: Removed broadcast_66_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:57:31 INFO BlockManagerInfo: Removed broadcast_65_piece0 on localhost:44647 in memory (size: 11.4 KiB, free: 912.2 MiB)
20/08/17 07:57:31 INFO BlockManagerInfo: Removed broadcast_61_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 07:57:31 INFO BlockManagerInfo: Removed broadcast_60_piece0 on localhost:44647 in memory (size: 11.6 KiB, free: 912.3 MiB)
20/08/17 07:57:31 INFO BlockManagerInfo: Removed broadcast_62_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.3 MiB)
20/08/17 07:57:31 INFO BlockManagerInfo: Removed broadcast_63_piece0 on localhost:44647 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 08:31:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:31:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:31:08 INFO FileSourceStrategy: Pruning directories with: 
20/08/17 08:31:08 INFO FileSourceStrategy: Pushed Filters: 
20/08/17 08:31:08 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#6535, None)) > 0)
20/08/17 08:31:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 292.8 KiB, free 911.7 MiB)
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 911.7 MiB)
20/08/17 08:31:09 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on localhost:44647 (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 08:31:09 INFO SparkContext: Created broadcast 67 from csv at NativeMethodAccessorImpl.java:0
20/08/17 08:31:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/08/17 08:31:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/08/17 08:31:09 INFO DAGScheduler: Got job 38 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 08:31:09 INFO DAGScheduler: Final stage: ResultStage 65 (csv at NativeMethodAccessorImpl.java:0)
20/08/17 08:31:09 INFO DAGScheduler: Parents of final stage: List()
20/08/17 08:31:09 INFO DAGScheduler: Missing parents: List()
20/08/17 08:31:09 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[296] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 10.7 KiB, free 911.7 MiB)
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 911.7 MiB)
20/08/17 08:31:09 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on localhost:44647 (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 08:31:09 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1200
20/08/17 08:31:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[296] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 08:31:09 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks
20/08/17 08:31:09 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 67, localhost, executor driver, partition 0, PROCESS_LOCAL, 7750 bytes)
20/08/17 08:31:09 INFO Executor: Running task 0.0 in stage 65.0 (TID 67)
20/08/17 08:31:09 INFO FileScanRDD: Reading File path: file:///home/emmanuel/R/Dissertation/input/cars_1.csv, range: 0-1303, partition values: [empty row]
20/08/17 08:31:09 INFO Executor: Finished task 0.0 in stage 65.0 (TID 67). 1586 bytes result sent to driver
20/08/17 08:31:09 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 67) in 39 ms on localhost (executor driver) (1/1)
20/08/17 08:31:09 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
20/08/17 08:31:09 INFO DAGScheduler: ResultStage 65 (csv at NativeMethodAccessorImpl.java:0) finished in 0.137 s
20/08/17 08:31:09 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 08:31:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
20/08/17 08:31:09 INFO DAGScheduler: Job 38 finished: csv at NativeMethodAccessorImpl.java:0, took 0.141612 s
20/08/17 08:31:09 INFO FileSourceStrategy: Pruning directories with: 
20/08/17 08:31:09 INFO FileSourceStrategy: Pushed Filters: 
20/08/17 08:31:09 INFO FileSourceStrategy: Post-Scan Filters: 
20/08/17 08:31:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 292.8 KiB, free 911.4 MiB)
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 911.3 MiB)
20/08/17 08:31:09 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on localhost:44647 (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 08:31:09 INFO SparkContext: Created broadcast 69 from csv at NativeMethodAccessorImpl.java:0
20/08/17 08:31:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/08/17 08:31:09 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/08/17 08:31:09 INFO DAGScheduler: Got job 39 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 08:31:09 INFO DAGScheduler: Final stage: ResultStage 66 (csv at NativeMethodAccessorImpl.java:0)
20/08/17 08:31:09 INFO DAGScheduler: Parents of final stage: List()
20/08/17 08:31:09 INFO DAGScheduler: Missing parents: List()
20/08/17 08:31:09 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[302] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 15.5 KiB, free 911.3 MiB)
20/08/17 08:31:09 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 911.3 MiB)
20/08/17 08:31:09 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on localhost:44647 (size: 7.9 KiB, free: 912.2 MiB)
20/08/17 08:31:09 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1200
20/08/17 08:31:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[302] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 08:31:09 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks
20/08/17 08:31:09 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 68, localhost, executor driver, partition 0, PROCESS_LOCAL, 7750 bytes)
20/08/17 08:31:09 INFO Executor: Running task 0.0 in stage 66.0 (TID 68)
20/08/17 08:31:09 INFO FileScanRDD: Reading File path: file:///home/emmanuel/R/Dissertation/input/cars_1.csv, range: 0-1303, partition values: [empty row]
20/08/17 08:31:09 INFO Executor: Finished task 0.0 in stage 66.0 (TID 68). 1734 bytes result sent to driver
20/08/17 08:31:09 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 68) in 71 ms on localhost (executor driver) (1/1)
20/08/17 08:31:09 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
20/08/17 08:31:09 INFO DAGScheduler: ResultStage 66 (csv at NativeMethodAccessorImpl.java:0) finished in 0.114 s
20/08/17 08:31:09 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 08:31:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
20/08/17 08:31:09 INFO DAGScheduler: Job 39 finished: csv at NativeMethodAccessorImpl.java:0, took 0.119976 s
20/08/17 08:31:10 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:11 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/08/17 08:31:11 INFO MicroBatchExecution: Checkpoint root output/checkpoint resolved to file:/home/emmanuel/R/Dissertation/output/checkpoint.
20/08/17 08:31:11 INFO CheckpointFileManager: Writing atomically to file:/home/emmanuel/R/Dissertation/output/checkpoint/metadata using temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/.metadata.08f3cb4f-0cc4-4f06-8dda-b00bc1f7dc52.tmp
20/08/17 08:31:11 INFO CheckpointFileManager: Renamed temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/.metadata.08f3cb4f-0cc4-4f06-8dda-b00bc1f7dc52.tmp to file:/home/emmanuel/R/Dissertation/output/checkpoint/metadata
20/08/17 08:31:11 INFO MicroBatchExecution: Starting stream__410a73ce_cef5_4e3d_a432_533f761e8ca1 [id = 451df69a-1082-4aca-922e-7972a2e4762b, runId = ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb]. Use file:/home/emmanuel/R/Dissertation/output/checkpoint to store the query checkpoint.
20/08/17 08:31:11 INFO FileStreamSourceLog: Set the compact interval to 10 [defaultCompactInterval: 10]
20/08/17 08:31:11 INFO FileStreamSource: maxFilesPerBatch = None, maxFileAgeMs = 604800000
20/08/17 08:31:11 INFO MicroBatchExecution: Using Source [FileStreamSource[file:/home/emmanuel/R/Dissertation/input]] from DataSourceV1 named 'FileSource[input/]' [DataSource(org.apache.spark.sql.SparkSession@7c85486d,csv,List(),Some(StructType(StructField(mpg,DoubleType,true), StructField(cyl,IntegerType,true), StructField(disp,DoubleType,true), StructField(hp,IntegerType,true), StructField(drat,DoubleType,true), StructField(wt,DoubleType,true), StructField(qsec,DoubleType,true), StructField(vs,IntegerType,true), StructField(am,IntegerType,true), StructField(gear,IntegerType,true), StructField(carb,IntegerType,true))),List(),None,Map(path -> input/, nullValue -> , inferSchema -> true, quote -> ", escape -> \, charset -> UTF-8, header -> true, delimiter -> ,),None)]
20/08/17 08:31:11 INFO MicroBatchExecution: Starting new streaming query.
20/08/17 08:31:11 INFO MicroBatchExecution: Stream started from {}
20/08/17 08:31:11 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/08/17 08:31:11 INFO CheckpointFileManager: Writing atomically to file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/0 using temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/.0.ae90d4a6-c9f1-49c2-9cc5-e2272ee22244.tmp
20/08/17 08:31:11 INFO CheckpointFileManager: Renamed temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/.0.ae90d4a6-c9f1-49c2-9cc5-e2272ee22244.tmp to file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/0
20/08/17 08:31:11 INFO FileStreamSource: Log offset set to 0 with 1 new files
20/08/17 08:31:11 INFO CheckpointFileManager: Writing atomically to file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/0 using temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/.0.058b0936-d733-4dec-8102-8d6b8259432c.tmp
20/08/17 08:31:11 INFO CheckpointFileManager: Renamed temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/.0.058b0936-d733-4dec-8102-8d6b8259432c.tmp to file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/0
20/08/17 08:31:11 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1597667471836,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))
20/08/17 08:31:11 INFO FileStreamSource: Processing 1 files from 0:0
20/08/17 08:31:11 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:12 INFO FileSourceStrategy: Pruning directories with: 
20/08/17 08:31:12 INFO FileSourceStrategy: Pushed Filters: 
20/08/17 08:31:12 INFO FileSourceStrategy: Post-Scan Filters: 
20/08/17 08:31:12 INFO FileSourceStrategy: Output Data Schema: struct<mpg: double, cyl: int, disp: double ... 1 more fields>
20/08/17 08:31:12 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 293.0 KiB, free 911.0 MiB)
20/08/17 08:31:12 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 911.0 MiB)
20/08/17 08:31:12 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on localhost:44647 (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 08:31:12 INFO SparkContext: Created broadcast 71 from start at NativeMethodAccessorImpl.java:0
20/08/17 08:31:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/08/17 08:31:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
20/08/17 08:31:12 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 08:31:12 INFO DAGScheduler: Final stage: ResultStage 67 (start at NativeMethodAccessorImpl.java:0)
20/08/17 08:31:12 INFO DAGScheduler: Parents of final stage: List()
20/08/17 08:31:12 INFO DAGScheduler: Missing parents: List()
20/08/17 08:31:12 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[304] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 08:31:12 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 150.5 KiB, free 910.9 MiB)
20/08/17 08:31:12 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 54.3 KiB, free 910.8 MiB)
20/08/17 08:31:12 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on localhost:44647 (size: 54.3 KiB, free: 912.1 MiB)
20/08/17 08:31:12 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1200
20/08/17 08:31:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[304] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 08:31:12 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks
20/08/17 08:31:12 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 69, localhost, executor driver, partition 0, PROCESS_LOCAL, 7750 bytes)
20/08/17 08:31:12 INFO Executor: Running task 0.0 in stage 67.0 (TID 69)
20/08/17 08:31:12 INFO FileScanRDD: Reading File path: file:///home/emmanuel/R/Dissertation/input/cars_1.csv, range: 0-1303, partition values: [empty row]
20/08/17 08:31:12 INFO CodeGenerator: Code generated in 27.516458 ms
20/08/17 08:31:12 INFO Executor: Finished task 0.0 in stage 67.0 (TID 69). 2605 bytes result sent to driver
20/08/17 08:31:12 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 69) in 157 ms on localhost (executor driver) (1/1)
20/08/17 08:31:12 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
20/08/17 08:31:12 INFO DAGScheduler: ResultStage 67 (start at NativeMethodAccessorImpl.java:0) finished in 0.203 s
20/08/17 08:31:12 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 08:31:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
20/08/17 08:31:12 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.213384 s
20/08/17 08:31:12 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]
20/08/17 08:31:12 INFO CheckpointFileManager: Writing atomically to output/_spark_metadata/0 using temp file output/_spark_metadata/.0.577c22b9-dfbd-480a-ac58-224ffe0cd94c.tmp
20/08/17 08:31:12 INFO CheckpointFileManager: Renamed temp file output/_spark_metadata/.0.577c22b9-dfbd-480a-ac58-224ffe0cd94c.tmp to output/_spark_metadata/0
20/08/17 08:31:12 INFO ManifestFileCommitProtocol: Committed batch 0
20/08/17 08:31:12 INFO FileFormatWriter: Write Job 96e753ef-ace8-4999-adac-d1babc6c9dfd committed.
20/08/17 08:31:12 INFO FileFormatWriter: Finished processing stats for write job 96e753ef-ace8-4999-adac-d1babc6c9dfd.
20/08/17 08:31:12 INFO CheckpointFileManager: Writing atomically to file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/0 using temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/.0.6c51c0cb-a062-4064-b734-207b2edb209d.tmp
20/08/17 08:31:12 INFO CheckpointFileManager: Renamed temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/.0.6c51c0cb-a062-4064-b734-207b2edb209d.tmp to file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/0
20/08/17 08:31:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:31:11.629Z",
  "batchId" : 0,
  "numInputRows" : 32,
  "processedRowsPerSecond" : 33.61344537815126,
  "durationMs" : {
    "addBatch" : 366,
    "getBatch" : 83,
    "latestOffset" : 182,
    "queryPlanning" : 125,
    "triggerExecution" : 950,
    "walCommit" : 80
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : null,
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 32,
    "processedRowsPerSecond" : 33.61344537815126
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:31:12 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
20/08/17 08:31:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:31:12.625Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 10,
    "triggerExecution" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:31:13 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:14 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:15 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.
20/08/17 08:31:16 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:17 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:31:18 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:31:19 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:20 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:21 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:31:22 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:31:23 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:31:23.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:31:24 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:25 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:26 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:27 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:28 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:29 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:30 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:31:31 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:31:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:33 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:31:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:31:33.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:31:34 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/08/17 08:31:35 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:36 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:37 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:40 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:41 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:31:43.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 9
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:31:44 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:31:45 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:47 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:53 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:31:53.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 9
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:31:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:31:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:00 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:32:01 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:32:04.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:32:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:10 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:11 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:32:12 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:14 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:32:14.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 10
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:32:15 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:17 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:18 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/08/17 08:32:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:22 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:24 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:32:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:32:25.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 11,
    "triggerExecution" : 12
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:32:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:32:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:28 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:33 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:32:34 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:35 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:32:35.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 10,
    "triggerExecution" : 17
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:32:36 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:32:37 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:32:38 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:32:39 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:32:40 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:32:41 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:32:42 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:32:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:45 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:32:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:32:46.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 8
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:32:47 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:32:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:32:56.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 8
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:32:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:32:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:00 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:33:01 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:33:06.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 8,
    "triggerExecution" : 9
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:33:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:10 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:11 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:33:12 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:15 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:16 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:33:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:33:16.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 8,
    "triggerExecution" : 10
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:33:17 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:22 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:33:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:33:27.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:33:28 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:34 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:35 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:36 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:37 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:37 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:33:37.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 8
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:33:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:40 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:41 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/08/17 08:33:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:44 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:33:45 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:33:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:47 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:33:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:33:48.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 8
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:33:49 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:33:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:33:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:33:59.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:34:00 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:34:01 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:09 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:34:09 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:34:09.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 15,
    "triggerExecution" : 16
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:34:10 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:11 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:12 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:15 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:17 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:18 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:34:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:20 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:34:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:34:20.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:34:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:22 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:34:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:28 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:30 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:34:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:34:30.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:34:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:32 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:34:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:34 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:34:35 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:36 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:37 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:34:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:40 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/08/17 08:34:40 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:34:40.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 11,
    "triggerExecution" : 11
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:34:41 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:45 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:46 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:34:47 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:34:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:34:51.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:34:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:54 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:34:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:34:58 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:34:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:00 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:35:01 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:35:02.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 12,
    "triggerExecution" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:35:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:05 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:35:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:10 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:11 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:12 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:13 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:35:13.000Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:35:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:15 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:17 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:19 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:35:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:22 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:35:23.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 8,
    "triggerExecution" : 11
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:35:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:28 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:35:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:34 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:35:34.002Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:35:35 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:35:36 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:37 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:38 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:35:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:40 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:35:42 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
20/08/17 08:35:42 INFO BlockManagerInfo: Removed broadcast_69_piece0 on localhost:44647 in memory (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 08:35:42 INFO BlockManagerInfo: Removed broadcast_68_piece0 on localhost:44647 in memory (size: 5.3 KiB, free: 912.2 MiB)
20/08/17 08:35:42 INFO BlockManagerInfo: Removed broadcast_67_piece0 on localhost:44647 in memory (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 08:35:42 INFO BlockManagerInfo: Removed broadcast_70_piece0 on localhost:44647 in memory (size: 7.9 KiB, free: 912.2 MiB)
20/08/17 08:35:42 INFO BlockManagerInfo: Removed broadcast_72_piece0 on localhost:44647 in memory (size: 54.3 KiB, free: 912.2 MiB)
20/08/17 08:35:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:35:44.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 8
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:35:45 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:47 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:35:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:54 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:35:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:35:54.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 8,
    "triggerExecution" : 11
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:35:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:35:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:00 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:01 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:03 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:36:05.001Z",
  "batchId" : 1,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 12,
    "triggerExecution" : 13
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:36:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:07 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:09 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:36:10 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:11 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:11 INFO CheckpointFileManager: Writing atomically to file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/1 using temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/.1.8d5db8a1-2a67-4982-8626-9c7f64b3cc0d.tmp
20/08/17 08:36:11 INFO CheckpointFileManager: Renamed temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/.1.8d5db8a1-2a67-4982-8626-9c7f64b3cc0d.tmp to file:/home/emmanuel/R/Dissertation/output/checkpoint/sources/0/1
20/08/17 08:36:11 INFO FileStreamSource: Log offset set to 1 with 1 new files
20/08/17 08:36:11 INFO CheckpointFileManager: Writing atomically to file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/1 using temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/.1.68111c47-3d08-468b-9f84-af949e08c2f1.tmp
20/08/17 08:36:11 INFO CheckpointFileManager: Renamed temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/.1.68111c47-3d08-468b-9f84-af949e08c2f1.tmp to file:/home/emmanuel/R/Dissertation/output/checkpoint/offsets/1
20/08/17 08:36:11 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1597667771023,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))
20/08/17 08:36:11 INFO FileStreamSource: Processing 1 files from 1:1
20/08/17 08:36:11 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:11 INFO FileSourceStrategy: Pruning directories with: 
20/08/17 08:36:11 INFO FileSourceStrategy: Pushed Filters: 
20/08/17 08:36:11 INFO FileSourceStrategy: Post-Scan Filters: 
20/08/17 08:36:11 INFO FileSourceStrategy: Output Data Schema: struct<mpg: double, cyl: int, disp: double ... 1 more fields>
20/08/17 08:36:11 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 293.0 KiB, free 911.4 MiB)
20/08/17 08:36:11 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 911.4 MiB)
20/08/17 08:36:11 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on localhost:44647 (size: 24.4 KiB, free: 912.2 MiB)
20/08/17 08:36:11 INFO SparkContext: Created broadcast 73 from start at NativeMethodAccessorImpl.java:0
20/08/17 08:36:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20/08/17 08:36:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
20/08/17 08:36:11 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/08/17 08:36:11 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)
20/08/17 08:36:11 INFO DAGScheduler: Parents of final stage: List()
20/08/17 08:36:11 INFO DAGScheduler: Missing parents: List()
20/08/17 08:36:11 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[306] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
20/08/17 08:36:11 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 150.5 KiB, free 911.2 MiB)
20/08/17 08:36:11 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 54.3 KiB, free 911.2 MiB)
20/08/17 08:36:11 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on localhost:44647 (size: 54.3 KiB, free: 912.2 MiB)
20/08/17 08:36:11 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1200
20/08/17 08:36:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[306] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/08/17 08:36:11 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks
20/08/17 08:36:11 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 70, localhost, executor driver, partition 0, PROCESS_LOCAL, 7750 bytes)
20/08/17 08:36:11 INFO Executor: Running task 0.0 in stage 68.0 (TID 70)
20/08/17 08:36:11 INFO FileScanRDD: Reading File path: file:///home/emmanuel/R/Dissertation/input/cars_2.csv, range: 0-1303, partition values: [empty row]
20/08/17 08:36:11 INFO Executor: Finished task 0.0 in stage 68.0 (TID 70). 2562 bytes result sent to driver
20/08/17 08:36:11 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 70) in 69 ms on localhost (executor driver) (1/1)
20/08/17 08:36:11 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
20/08/17 08:36:11 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.115 s
20/08/17 08:36:11 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 08:36:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
20/08/17 08:36:11 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.117549 s
20/08/17 08:36:11 INFO CheckpointFileManager: Writing atomically to output/_spark_metadata/1 using temp file output/_spark_metadata/.1.b6f7dbcc-423d-4b05-9dbf-80f0196cb056.tmp
20/08/17 08:36:11 INFO CheckpointFileManager: Renamed temp file output/_spark_metadata/.1.b6f7dbcc-423d-4b05-9dbf-80f0196cb056.tmp to output/_spark_metadata/1
20/08/17 08:36:11 INFO ManifestFileCommitProtocol: Committed batch 1
20/08/17 08:36:11 INFO FileFormatWriter: Write Job 776309f2-0c65-43cb-b672-6509d0f972bf committed.
20/08/17 08:36:11 INFO FileFormatWriter: Finished processing stats for write job 776309f2-0c65-43cb-b672-6509d0f972bf.
20/08/17 08:36:11 INFO CheckpointFileManager: Writing atomically to file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/1 using temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/.1.f087d06a-d124-41ba-ba02-e19bcff1b9f1.tmp
20/08/17 08:36:11 INFO CheckpointFileManager: Renamed temp file file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/.1.f087d06a-d124-41ba-ba02-e19bcff1b9f1.tmp to file:/home/emmanuel/R/Dissertation/output/checkpoint/commits/1
20/08/17 08:36:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:36:11.001Z",
  "batchId" : 1,
  "numInputRows" : 32,
  "inputRowsPerSecond" : 32.0,
  "processedRowsPerSecond" : 77.29468599033817,
  "durationMs" : {
    "addBatch" : 250,
    "getBatch" : 19,
    "latestOffset" : 22,
    "queryPlanning" : 40,
    "triggerExecution" : 414,
    "walCommit" : 28
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 32,
    "inputRowsPerSecond" : 32.0,
    "processedRowsPerSecond" : 77.29468599033817
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:36:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:36:12.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:36:13 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:14 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:15 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:16 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:17 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:18 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:19 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:36:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:22 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:36:22.002Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:36:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:28 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:32 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:36:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:36:33.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:36:34 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:35 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:37 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:40 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:41 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:36:44.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:36:45 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:47 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:49 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:36:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:55 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:36:55.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:36:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:36:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:00 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:01 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:37:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:37:06.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:37:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:10 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:37:11 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:12 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:14 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:15 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:17 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:37:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:37:17.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 9
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:37:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:20 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:37:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:22 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:24 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/08/17 08:37:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:27 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:37:27 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:37:27.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 8
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:37:28 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:37:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:32 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:37:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:34 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:37:35 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:36 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:37 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:37:38.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:37:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:40 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:41 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:44 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:45 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:47 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:37:48.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:37:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:37:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:37:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:56 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:57 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:37:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:37:58 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:37:58.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:37:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:00 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:01 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
20/08/17 08:38:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:04 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:06 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:38:08.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:38:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:10 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:38:11 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:12 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:38:15 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:17 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
20/08/17 08:38:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:18 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:38:18.001Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:38:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:22 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:25 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:26 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:28 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/08/17 08:38:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "451df69a-1082-4aca-922e-7972a2e4762b",
  "runId" : "ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb",
  "name" : "stream__410a73ce_cef5_4e3d_a432_533f761e8ca1",
  "timestamp" : "2020-08-17T12:38:29.000Z",
  "batchId" : 2,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "latestOffset" : 8,
    "triggerExecution" : 11
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/home/emmanuel/R/Dissertation/input]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "FileSink[output/]",
    "numOutputRows" : -1
  }
}
20/08/17 08:38:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/08/17 08:38:30 INFO DAGScheduler: Asked to cancel job group ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb
20/08/17 08:38:30 INFO DAGScheduler: Asked to cancel job group ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb
20/08/17 08:38:30 INFO MicroBatchExecution: Query stream__410a73ce_cef5_4e3d_a432_533f761e8ca1 [id = 451df69a-1082-4aca-922e-7972a2e4762b, runId = ab50b8b4-ee85-4c78-8113-d5f8cd9b91eb] was stopped
20/08/17 08:45:37 INFO SparkContext: Invoking stop() from shutdown hook
20/08/17 08:45:37 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
20/08/17 08:45:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/08/17 08:45:38 INFO MemoryStore: MemoryStore cleared
20/08/17 08:45:38 INFO BlockManager: BlockManager stopped
20/08/17 08:45:38 INFO BlockManagerMaster: BlockManagerMaster stopped
20/08/17 08:45:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/08/17 08:45:38 INFO SparkContext: Successfully stopped SparkContext
20/08/17 08:45:38 INFO ShutdownHookManager: Shutdown hook called
20/08/17 08:45:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-74a4eab9-6367-47c8-91d6-5f24d9b85489
20/08/17 08:45:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ce82bfc-a42a-417a-8c7f-0cdd69c69ed5
20/08/17 11:58:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/08/17 11:58:24 INFO SecurityManager: Changing view acls to: emmanuel
20/08/17 11:58:24 INFO SecurityManager: Changing modify acls to: emmanuel
20/08/17 11:58:24 INFO SecurityManager: Changing view acls groups to: 
20/08/17 11:58:24 INFO SecurityManager: Changing modify acls groups to: 
20/08/17 11:58:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(emmanuel); groups with view permissions: Set(); users  with modify permissions: Set(emmanuel); groups with modify permissions: Set()
20/08/17 11:58:25 INFO HiveConf: Found configuration file file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 11:58:26 INFO SparkContext: Running Spark version 3.0.0
20/08/17 11:58:26 INFO ResourceUtils: ==============================================================
20/08/17 11:58:26 INFO ResourceUtils: Resources for spark.driver:

20/08/17 11:58:26 INFO ResourceUtils: ==============================================================
20/08/17 11:58:26 INFO SparkContext: Submitted application: sparklyr
20/08/17 11:58:26 INFO SecurityManager: Changing view acls to: emmanuel
20/08/17 11:58:26 INFO SecurityManager: Changing modify acls to: emmanuel
20/08/17 11:58:26 INFO SecurityManager: Changing view acls groups to: 
20/08/17 11:58:26 INFO SecurityManager: Changing modify acls groups to: 
20/08/17 11:58:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(emmanuel); groups with view permissions: Set(); users  with modify permissions: Set(emmanuel); groups with modify permissions: Set()
20/08/17 11:58:26 INFO Utils: Successfully started service 'sparkDriver' on port 41741.
20/08/17 11:58:26 INFO SparkEnv: Registering MapOutputTracker
20/08/17 11:58:26 INFO SparkEnv: Registering BlockManagerMaster
20/08/17 11:58:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/08/17 11:58:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/08/17 11:58:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/08/17 11:58:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c920ec9b-2b7d-4166-bde8-e75b44622746
20/08/17 11:58:27 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/08/17 11:58:27 INFO SparkEnv: Registering OutputCommitCoordinator
20/08/17 11:58:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/08/17 11:58:27 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
20/08/17 11:58:28 INFO SparkContext: Added JAR file:/home/emmanuel/R/x86_64-pc-linux-gnu-library/3.4/sparklyr/java/sparklyr-3.0-2.12.jar at spark://localhost:41741/jars/sparklyr-3.0-2.12.jar with timestamp 1597679908027
20/08/17 11:58:28 INFO Executor: Starting executor ID driver on host localhost
20/08/17 11:58:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32903.
20/08/17 11:58:28 INFO NettyBlockTransferService: Server created on localhost:32903
20/08/17 11:58:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/08/17 11:58:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 32903, None)
20/08/17 11:58:28 INFO BlockManagerMasterEndpoint: Registering block manager localhost:32903 with 912.3 MiB RAM, BlockManagerId(driver, localhost, 32903, None)
20/08/17 11:58:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 32903, None)
20/08/17 11:58:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 32903, None)
20/08/17 11:58:29 INFO SharedState: loading hive config file: file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 11:58:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/emmanuel/R/Dissertation/spark-warehouse').
20/08/17 11:58:29 INFO SharedState: Warehouse path is 'file:/home/emmanuel/R/Dissertation/spark-warehouse'.
20/08/17 11:58:34 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/08/17 11:58:34 INFO HiveConf: Found configuration file file:/home/emmanuel/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/08/17 11:58:36 INFO SessionState: Created HDFS directory: /tmp/hive/emmanuel/2a247a8e-a9e8-4476-922b-505735c4f340
20/08/17 11:58:36 INFO SessionState: Created local directory: /tmp/emmanuel/2a247a8e-a9e8-4476-922b-505735c4f340
20/08/17 11:58:36 INFO SessionState: Created HDFS directory: /tmp/hive/emmanuel/2a247a8e-a9e8-4476-922b-505735c4f340/_tmp_space.db
20/08/17 11:58:36 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is file:/home/emmanuel/R/Dissertation/spark-warehouse
20/08/17 11:58:37 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/08/17 11:58:37 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/08/17 11:58:37 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/08/17 11:58:37 INFO ObjectStore: ObjectStore, initialize called
20/08/17 11:58:37 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/08/17 11:58:37 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/08/17 11:58:41 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/08/17 11:58:44 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/08/17 11:58:44 INFO ObjectStore: Initialized ObjectStore
20/08/17 11:58:44 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/08/17 11:58:44 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore emmanuel@127.0.1.1
20/08/17 11:58:44 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/08/17 11:58:45 INFO HiveMetaStore: Added admin role in metastore
20/08/17 11:58:45 INFO HiveMetaStore: Added public role in metastore
20/08/17 11:58:45 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/08/17 11:58:45 INFO HiveMetaStore: 0: get_all_functions
20/08/17 11:58:45 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_all_functions	
20/08/17 11:58:46 INFO HiveMetaStore: 0: get_database: default
20/08/17 11:58:46 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 11:58:46 INFO HiveMetaStore: 0: get_database: global_temp
20/08/17 11:58:46 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/08/17 11:58:46 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/08/17 11:58:46 INFO HiveMetaStore: 0: get_database: default
20/08/17 11:58:46 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 11:58:46 INFO HiveMetaStore: 0: get_database: default
20/08/17 11:58:46 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_database: default	
20/08/17 11:58:46 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/08/17 11:58:46 INFO audit: ugi=emmanuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/08/17 11:58:49 INFO CodeGenerator: Code generated in 518.742702 ms
20/08/17 11:58:49 INFO CodeGenerator: Code generated in 35.835798 ms
20/08/17 11:58:50 INFO SparkContext: Starting job: count at utils.scala:114
20/08/17 11:58:50 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/08/17 11:58:50 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/08/17 11:58:50 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/08/17 11:58:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/08/17 11:58:50 INFO DAGScheduler: Missing parents: List()
20/08/17 11:58:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/08/17 11:58:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/08/17 11:58:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/08/17 11:58:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:32903 (size: 5.0 KiB, free: 912.3 MiB)
20/08/17 11:58:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/08/17 11:58:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/08/17 11:58:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/08/17 11:58:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/08/17 11:58:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/08/17 11:58:51 INFO Executor: Fetching spark://localhost:41741/jars/sparklyr-3.0-2.12.jar with timestamp 1597679908027
20/08/17 11:58:51 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:41741 after 63 ms (0 ms spent in bootstraps)
20/08/17 11:58:51 INFO Utils: Fetching spark://localhost:41741/jars/sparklyr-3.0-2.12.jar to /tmp/spark-a2822a8f-328a-40d0-adab-8e9b03185c13/userFiles-312f913e-8b38-44aa-b804-99581e53721a/fetchFileTemp5445721109795759520.tmp
20/08/17 11:58:52 INFO Executor: Adding file:/tmp/spark-a2822a8f-328a-40d0-adab-8e9b03185c13/userFiles-312f913e-8b38-44aa-b804-99581e53721a/sparklyr-3.0-2.12.jar to class loader
20/08/17 11:58:53 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/08/17 11:58:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
20/08/17 11:58:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/08/17 11:58:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1605 ms on localhost (executor driver) (1/1)
20/08/17 11:58:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/08/17 11:58:53 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2.325 s
20/08/17 11:58:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/08/17 11:58:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/08/17 11:58:53 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2.664880 s
